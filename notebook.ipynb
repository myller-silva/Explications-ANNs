{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "import os\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from statistics import mean, stdev \n",
    "from typing import List\n",
    "import docplex.mp.model as mp\n",
    "from docplex.mp.constr import LinearConstraint\n",
    "import tensorflow as tf\n",
    "import cplex\n",
    "from cplex import infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fischetti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codify_network_fischetti(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "    bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(\n",
    "                    A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                )\n",
    "                mdl.add_indicator(a[j], y[j] <= 0, 1)\n",
    "                mdl.add_indicator(a[j], s[j] <= 0, 0)\n",
    "\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub_y = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.maximize(s[j])\n",
    "                mdl.solve()\n",
    "                ub_s = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub_y)\n",
    "                s[j].set_ub(ub_s)\n",
    "\n",
    "                bounds.append([-ub_s, ub_y])\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] ==\n",
    "                                   y[j], ctname=f\"c_{i}_{j}\")\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.minimize(y[j])\n",
    "                mdl.solve()\n",
    "                lb = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub)\n",
    "                y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "                bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds, bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaxado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codify_network_fischetti_relaxed(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds=[]\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):  # para cada camada\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):  # para cada neuronio da camada\n",
    "            if i != len(layers) - 1:  # se não for a última camada(camada de saída)\n",
    "                m_less, m_more = bounds[j]\n",
    "\n",
    "                y[j].set_ub(-m_less)\n",
    "                s[j].set_ub(m_more)\n",
    "                if m_more <= 0:\n",
    "                    mdl.add_constraint(y[j] == 0)\n",
    "                    continue\n",
    "\n",
    "                if m_less >= 0:\n",
    "                    mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                    continue\n",
    "\n",
    "                if m_less < 0 and m_more > 0:\n",
    "                    mdl.add_constraint(\n",
    "                        A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                    )\n",
    "                    mdl.add_constraint(y[j] <= m_more * (1 - a[j]))\n",
    "                    mdl.add_constraint(s[j] <= -m_less * a[j])\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                lb, ub = output_bounds_binary_variables[j]\n",
    "                y[j].set_lb(lb)\n",
    "                y[j].set_ub(ub)\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tjeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codify_network_tjeng(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            mdl.maximize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            ub = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if ub <= 0 and i != len(layers) - 1:\n",
    "                # print(\"ENTROU, o ub é negativo, logo y = 0\")\n",
    "                mdl.add_constraint(y[j] == 0, ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            mdl.minimize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            lb = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if lb >= 0 and i != len(layers) - 1:\n",
    "                # print(\"ENTROU, o lb >= 0, logo y = Wx + b\")\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] ==\n",
    "                                   y[j], ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(y[j] <= A[j, :] @ x +\n",
    "                                   b[j] - lb * (1 - a[j]))\n",
    "                mdl.add_constraint(y[j] >= A[j, :] @ x + b[j])\n",
    "                mdl.add_constraint(y[j] <= ub * a[j])\n",
    "\n",
    "                # mdl.maximize(y[j])\n",
    "                # mdl.solve()\n",
    "                # ub_y = mdl.solution.get_objective_value()\n",
    "                # mdl.remove_objective()\n",
    "                # y[j].set_ub(ub_y)\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                # y[j].set_ub(ub)\n",
    "                # y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain and Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_and_bounds_inputs(dataframe):\n",
    "    domain = []\n",
    "    bounds = []\n",
    "    for column in dataframe.columns[:-1]:\n",
    "        if len(dataframe[column].unique()) == 2:\n",
    "            domain.append(\"B\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        elif np.any(\n",
    "            dataframe[column].unique().astype(np.int64)\n",
    "            != dataframe[column].unique().astype(np.float64)\n",
    "        ):\n",
    "            domain.append(\"C\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        else:\n",
    "            domain.append(\"I\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "\n",
    "    return domain, bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codify Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X ---- E**\n",
    "\n",
    "x1 = 1 ∧ x2 = 3 ∧ F ∧ ¬E  \n",
    "*INSATISFÁTIVEL*\n",
    "\n",
    "x1 ≥ 0 ∧ x1 ≤ 100 ∧ x2 = 3 ∧ F ∧ ¬E  \n",
    "*INSATISFÁTIVEL* → x1 não é relevante,  \n",
    "*SATISFATÍVEL* → x1 é relevante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codify_network(model, dataframe, method, relaxe_constraints):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:,\n",
    "                                          0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(\n",
    "                    mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(\n",
    "                    mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            decision_variables.append(\n",
    "                mdl.binary_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        mdl, output_bounds, bounds = codify_network_fischetti(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                a.set_vartype(\"Integer\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds, bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaxado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codify_network_relaxed(\n",
    "    model, dataframe, method, relaxe_constraints, output_bounds_binary_variables, bounds\n",
    "):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:,\n",
    "                                          0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            input_variables.append(\n",
    "                mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            # if domain_input[i] == \"C\":\n",
    "            #     input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            # elif domain_input[i] == \"I\":\n",
    "            #     input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            # elif domain_input[i] == \"B\":\n",
    "            #     input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # decision_variables.append(mdl.binary_var_list(weights.shape[1], name='a', lb=0, ub=1, key_format=f\"_{i}_%s\"))\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        # modificar depois para utilizar bounds precisos\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        # mdl, output_bounds = codify_network_fischetti(mdl, layers, input_variables, auxiliary_variables, intermediate_variables, decision_variables, output_variables)\n",
    "        mdl, output_bounds = codify_network_fischetti_relaxed(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "            output_bounds_binary_variables,\n",
    "            bounds=bounds\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                # a.set_vartype('Integer')\n",
    "                a.set_vartype(\"Continuous\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            x.set_vartype(\"Continuous\")\n",
    "            # if domain_input[i] == \"I\":\n",
    "            #     x.set_vartype(\"Integer\")\n",
    "            # elif domain_input[i] == \"B\":\n",
    "            #     x.set_vartype(\"Binary\")\n",
    "            # elif domain_input[i] == \"C\":\n",
    "            #     x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_output_constraints_fischetti(\n",
    "    mdl, output_variables, network_output, binary_variables\n",
    "):\n",
    "    # print(binary_variables)\n",
    "    variable_output = output_variables[network_output]\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            p = binary_variables[aux_var]\n",
    "            aux_var += 1\n",
    "            mdl.add_indicator(p, variable_output <= output, 1)\n",
    "\n",
    "    return mdl\n",
    "\n",
    "def insert_output_constraints_tjeng(\n",
    "    mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    upper_bounds_diffs = (\n",
    "        output_bounds[network_output][1] - np.array(output_bounds)[:, 0]\n",
    "    )  # Output i: oi - oj <= u1 = ui - lj\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            ub = upper_bounds_diffs[i]\n",
    "            z = binary_variables[aux_var]\n",
    "            mdl.add_constraint(variable_output - output - ub * (1 - z) <= 0)\n",
    "            aux_var += 1\n",
    "\n",
    "    return mdl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimal_explanation(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    ") -> List[LinearConstraint]:\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\")\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "    # todo: salvar modelo durante a explicação\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None:\n",
    "            mdl.add_constraint(constraint)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaxado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanation_relaxed(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    "    delta=0.1,\n",
    ") -> List[LinearConstraint]:\n",
    "    # todo: output_bounds só é relevante se o metodo for tjeng\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\")\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "    # todo: salvar modelo durante a explicação\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        x = constraint.get_left_expr()\n",
    "        v = constraint.get_right_expr()\n",
    "\n",
    "        constraint_left = mdl.add_constraint(v - delta <= x)\n",
    "        constraint_right = mdl.add_constraint(x <= v + delta)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None:\n",
    "            mdl.add_constraint(constraint)\n",
    "            mdl.remove_constraint(constraint_left)\n",
    "            mdl.remove_constraint(constraint_right)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede(dir_path: str, num_classes: int, n_neurons: int, n_hidden_layers: int):\n",
    "    data_train = pd.read_csv(dir_path + \"\\\\\" + \"train.csv\").to_numpy()\n",
    "    data_test = pd.read_csv(dir_path + \"\\\\\" + \"test.csv\").to_numpy()\n",
    "\n",
    "    x_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "    x_test, y_test = data_test[:, :-1], data_test[:, -1]\n",
    "\n",
    "    y_train_ohe = tf.keras.utils.to_categorical(\n",
    "        y_train, num_classes=num_classes)\n",
    "    y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=[x_train.shape[1]]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        dir_path, \"models\", f\"model_{n_hidden_layers}layers_{n_neurons}neurons.h5\"\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    ck = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor=\"val_accuracy\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train_ohe,\n",
    "        batch_size=4,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test_ohe),\n",
    "        verbose=2,\n",
    "        callbacks=[ck, es],\n",
    "    )\n",
    "    print(f\"Tempo de Treinamento: {time()-start}\")\n",
    "\n",
    "    # salvar modelo\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # avaliar modelo com os dados de treinamento\n",
    "    print(\"Resultado Treinamento\")\n",
    "    model.evaluate(x_train, y_train_ohe, verbose=2)\n",
    "\n",
    "    # avaliar modelo com os dados de teste\n",
    "    print(\"Resultado Teste\")\n",
    "    model.evaluate(x_test, y_test_ohe, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede_com_dataset_iris(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\iris\"\n",
    "    num_classes = 3\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_digits(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\digits\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_wine(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\wine\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicar Instância\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(\n",
    "    initial_network, dataset: dict, configuration: dict, instance_index: int\n",
    ") -> List[LinearConstraint]:\n",
    "\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "    method = configuration[\"method\"]\n",
    "\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = initial_network\n",
    "\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "\n",
    "    mdl_aux = mdl_milp_with_binary_variable.clone()\n",
    "\n",
    "    explanation = get_minimal_explanation(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables,\n",
    "    )\n",
    "\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance_relaxed(\n",
    "    initial_network,\n",
    "    initial_network_relaxed,\n",
    "    dataset: dict,\n",
    "    configuration: dict,\n",
    "    instance_index: int,\n",
    "    delta=1,\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = initial_network\n",
    "\n",
    "    model_milp_relaxed, output_bounds_relaxed = initial_network_relaxed\n",
    "\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "\n",
    "    mdl_aux = model_milp_relaxed.clone()\n",
    "\n",
    "    explanation = get_explanation_relaxed(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        # output_bounds_binary_variables = output_bounds_relaxed\n",
    "        output_bounds=output_bounds_binary_variables,\n",
    "        delta=delta,\n",
    "    )\n",
    "\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"dir_path\": \"datasets/digits\",\n",
    "        \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "        \"n_classes\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"dir_path\": \"datasets/digits\",\n",
    "        \"model\": \"models/model_2layers_20neurons.h5\",\n",
    "        \"n_classes\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"dir_path\": \"datasets/digits\",\n",
    "        \"model\": \"models/model_3layers_20neurons.h5\",\n",
    "        \"n_classes\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"dir_path\": \"datasets/digits\",\n",
    "        \"model\": \"models/model_4layers_20neurons.h5\",\n",
    "        \"n_classes\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"dir_path\": \"datasets/digits\",\n",
    "        \"model\": \"models/model_5layers_20neurons.h5\",\n",
    "        \"n_classes\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"dir_path\": \"datasets/iris\",\n",
    "        \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "        \"n_classes\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"dir_path\": \"datasets/iris\",\n",
    "        \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "        \"n_classes\": 3,\n",
    "    },\n",
    "]\n",
    "configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = 2\n",
    "\n",
    "dir_path, n_classes, model = (\n",
    "    datasets[dataset_index][\"dir_path\"],\n",
    "    datasets[dataset_index][\"n_classes\"],\n",
    "    datasets[dataset_index][\"model\"],\n",
    ")\n",
    "\n",
    "method = configurations[0][\"method\"]\n",
    "relaxe_constraints = configurations[0][\"relaxe_constraints\"]\n",
    "data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "data = data_train._append(data_test)\n",
    "model = tf.keras.models.load_model(f\"{dir_path}/{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diretório dos resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_do_diretorio = os.getcwd()\n",
    "nome_do_arquivo = (\n",
    "    f\"{datasets[dataset_index]['dir_path']}/{datasets[dataset_index]['model']}\"\n",
    ")\n",
    "caminho_completo = f\"{caminho_do_diretorio}/resultados/{nome_do_arquivo}\"\n",
    "if not os.path.exists(caminho_completo):\n",
    "    os.makedirs(caminho_completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos MILP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo MILP Original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: pegar output_bounds_binary_variables e bounds a partir do modelo já pronto, ao inves de salvar durante a modelagem\n",
    "# todo: modificar para verificar se o modelo já está codificado antes de codificar(ou seja, ler o arquivo .lp)\n",
    "initial_network = codify_network(model, data, method, relaxe_constraints)\n",
    "(\n",
    "    mdl_milp_with_binary_variable,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds,\n",
    ") = initial_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo MILP Relaxado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_network_relaxed = codify_network_relaxed(\n",
    "    model,\n",
    "    data,\n",
    "    method,\n",
    "    relaxe_constraints,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds=bounds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar Modelos MILPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_milp_as_lp(mdl: mp.Model, file: str):\n",
    "  mdl.export_as_lp(f\"{file}\")\n",
    "  mdl.export_as_lp_string(f\"{file}_string\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvar modelo MILP original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mylle\\\\OneDrive\\\\Documentos\\\\GitHub\\\\TestesTypescript\\\\Explications-ANNs/resultados/datasets/digits/models/model_3layers_20neurons.h5/original.lp'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_milp_with_binary_variable.export_as_lp(f\"{caminho_completo}/original.lp\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvar modelo MILP relaxado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mylle\\\\OneDrive\\\\Documentos\\\\GitHub\\\\TestesTypescript\\\\Explications-ANNs/resultados/datasets/digits/models/model_3layers_20neurons.h5/relaxed.lp'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mdl_relaxed, output_bounds_relaxed) = initial_network_relaxed\n",
    "mdl_relaxed.export_as_lp(f\"{caminho_completo}/relaxed.lp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrir Modelos MILPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abrir Modelo MILP Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_do_arquivo_lp = f\"{caminho_completo}/original.lp\"\n",
    "model_read = cplex.Cplex(caminho_do_arquivo_lp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abrir Modelo MILP Relaxado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning, line 1134: Name 'o_0' does not exist.\n",
      "Warning, line 1135: Name 'o_1' does not exist.\n",
      "Warning, line 1136: Name 'o_2' does not exist.\n",
      "Warning, line 1137: Name 'o_3' does not exist.\n",
      "Warning, line 1138: Name 'o_4' does not exist.\n",
      "Warning, line 1139: Name 'o_5' does not exist.\n",
      "Warning, line 1140: Name 'o_6' does not exist.\n",
      "Warning, line 1141: Name 'o_7' does not exist.\n",
      "Warning, line 1142: Name 'o_8' does not exist.\n",
      "Warning, line 1143: Name 'o_9' does not exist.\n"
     ]
    }
   ],
   "source": [
    "caminho_do_arquivo_lp = f\"{caminho_completo}/relaxed.lp\"\n",
    "model_read = cplex.Cplex(caminho_do_arquivo_lp)\n",
    "# initial_network_relaxed = (model_read, output_bounds_relaxed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executar Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_instance(\n",
    "    path: str, instance_index: int, delta=0.1, use_milp_original=False\n",
    "):\n",
    "    # todo: modificar para salvar no mesmo dataframe de resultados, independente do delta\n",
    "    directory = f\"{path}/delta{delta}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file = f\"{directory}/df.csv\"\n",
    "    if os.path.exists(file):\n",
    "        resultados = pd.read_csv(file)\n",
    "    else:\n",
    "        resultados = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"instance_index\",\n",
    "                \"tempo_original\",\n",
    "                \"tempo_relaxado\",\n",
    "                \"tempo_relaxado_global\",\n",
    "                \"len_original\",\n",
    "                \"len_relaxado\",\n",
    "                \"len_relaxado_global\",\n",
    "                \"delta\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(instance_index)\n",
    "    (\n",
    "        tempo_original, \n",
    "        len_original, \n",
    "        explanation\n",
    "    ) = [None] * 3\n",
    "    if use_milp_original:\n",
    "        # explain_instance original\n",
    "        start_time = time()\n",
    "        explanation = explain_instance(\n",
    "            initial_network=initial_network,\n",
    "            dataset=datasets[dataset_index],\n",
    "            configuration=configurations[0],\n",
    "            instance_index=instance_index,\n",
    "        )\n",
    "        end_time = time()\n",
    "        tempo_original = end_time - start_time\n",
    "        len_original = len(explanation)\n",
    "\n",
    "    # explain_instance_relaxed local\n",
    "    start_time = time()\n",
    "    explanation_relaxed = explain_instance_relaxed(\n",
    "        initial_network=initial_network,\n",
    "        initial_network_relaxed=initial_network_relaxed,\n",
    "        dataset=datasets[dataset_index],\n",
    "        configuration=configurations[0],\n",
    "        instance_index=instance_index,\n",
    "        delta=delta,\n",
    "    )\n",
    "\n",
    "    end_time = time()\n",
    "    tempo_relaxado = end_time - start_time\n",
    "    len_relaxado = len(explanation_relaxed)\n",
    "\n",
    "    # explain_instance_relaxed global\n",
    "    start_time = time()\n",
    "    explanation_relaxed_global = explain_instance_relaxed(\n",
    "        initial_network=initial_network,\n",
    "        initial_network_relaxed=initial_network_relaxed,\n",
    "        dataset=datasets[dataset_index],\n",
    "        configuration=configurations[0],\n",
    "        instance_index=instance_index,\n",
    "        delta=1,  # global\n",
    "    )\n",
    "\n",
    "    end_time = time()\n",
    "    tempo_relaxado_global = end_time - start_time\n",
    "    len_relaxado_global = len(explanation_relaxed_global)\n",
    "\n",
    "    resultados.loc[len(resultados)] = [\n",
    "        instance_index,\n",
    "        tempo_original,\n",
    "        tempo_relaxado,\n",
    "        tempo_relaxado_global,\n",
    "        len_original,\n",
    "        len_relaxado,\n",
    "        len_relaxado_global,\n",
    "        delta,\n",
    "    ]\n",
    "\n",
    "    # salvar\n",
    "    resultados.to_csv(f\"{directory}/df.csv\", index=False)\n",
    "    return [explanation, explanation_relaxed, explanation_relaxed_global]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(path: str, qtd=10, delta=0.1, use_milp_original=False):\n",
    "    directory = f\"{path}/delta{delta}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file = f\"{directory}/df.csv\"\n",
    "    if os.path.exists(file):\n",
    "        resultados = pd.read_csv(file)\n",
    "    else:\n",
    "        resultados = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"instance_index\",\n",
    "                \"tempo_original\",\n",
    "                \"tempo_relaxado\",\n",
    "                \"tempo_relaxado_global\",\n",
    "                \"len_original\",\n",
    "                \"len_relaxado\",\n",
    "                \"len_relaxado_global\",\n",
    "                \"delta\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    len_resultados = len(resultados)\n",
    "    for instance_index in range(len_resultados, len_resultados + qtd):\n",
    "\n",
    "        print(instance_index)\n",
    "        # todo: ao inves de passar o caminho do dataframe para explicar uma instancia, passar o dataframe em si, tanto no explain_instance quanto no explain_instance_relaxed\n",
    "        if use_milp_original:\n",
    "            # explain_instance original\n",
    "            start_time = time()\n",
    "            explanation = explain_instance(\n",
    "                initial_network=initial_network,\n",
    "                dataset=datasets[dataset_index],\n",
    "                configuration=configurations[0],\n",
    "                instance_index=instance_index,\n",
    "            )\n",
    "            end_time = time()\n",
    "            tempo_original = end_time - start_time\n",
    "            len_original = len(explanation)\n",
    "\n",
    "        # explain_instance_relaxed local\n",
    "        start_time = time()\n",
    "        explanation_relaxed = explain_instance_relaxed(\n",
    "            initial_network=initial_network,\n",
    "            initial_network_relaxed=initial_network_relaxed,\n",
    "            dataset=datasets[dataset_index],\n",
    "            configuration=configurations[0],\n",
    "            instance_index=instance_index,\n",
    "            delta=delta,\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        tempo_relaxado = end_time - start_time\n",
    "        len_relaxado = len(explanation_relaxed)\n",
    "\n",
    "        # explain_instance_relaxed global\n",
    "        start_time = time()\n",
    "        explanation_relaxed_global = explain_instance_relaxed(\n",
    "            initial_network=initial_network,\n",
    "            initial_network_relaxed=initial_network_relaxed,\n",
    "            dataset=datasets[dataset_index],\n",
    "            configuration=configurations[0],\n",
    "            instance_index=instance_index,\n",
    "            delta=1,  #global\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        tempo_relaxado_global = end_time - start_time\n",
    "        len_relaxado_global = len(explanation_relaxed_global)\n",
    "\n",
    "        resultados.loc[len(resultados)] = [\n",
    "            instance_index,\n",
    "            tempo_original if use_milp_original else None,\n",
    "            tempo_relaxado,\n",
    "            tempo_relaxado_global,\n",
    "            len_original if use_milp_original else None,\n",
    "            len_relaxado,\n",
    "            len_relaxado_global,\n",
    "            delta,\n",
    "        ]\n",
    "\n",
    "        # # salvar\n",
    "        resultados.to_csv(f\"{directory}/df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 76ms/step\n"
     ]
    }
   ],
   "source": [
    "path = f\"{caminho_completo}\"\n",
    "instance_index = 8\n",
    "delta = 0.9\n",
    "\n",
    "# benchmark(path=path, qtd=1, delta=delta, use_milp_original=False)\n",
    "(original, relaxed, relaxed_global) = benchmark_instance(\n",
    "    path=path, \n",
    "    instance_index=instance_index,\n",
    "    delta=delta, \n",
    "    use_milp_original=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def exibir_imagem_com_explicacao(df_instancia, instancia_index, explanation):\n",
    "    # Obter os pixels da instância específica\n",
    "    instancia = df_instancia.loc[instancia_index].values.reshape(8, 8)  # Supondo que seja uma imagem 28x28\n",
    "    \n",
    "    # Criar o objeto Figure e o objeto Axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Exibir a imagem da instância\n",
    "    ax.imshow(instancia, cmap='gray')  # Supondo que seja uma imagem em tons de cinza\n",
    "\n",
    "    # Iterar sobre as coordenadas da explicação e desenhar retângulos\n",
    "    for coord in explanation:\n",
    "        x, y, w, h = coord  # Supondo que cada coordenada é uma tupla (x, y, largura, altura)\n",
    "        # Adicionar um retângulo à imagem para destacar a região relevante\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Exibir a imagem com as regiões relevantes destacadas\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que você tenha um DataFrame chamado df_instancia contendo as instâncias de imagem\n",
    "# e um DataFrame chamado explicacao contendo as explicações para cada instância\n",
    "# df_instancia = pd.read_csv(\"\")\n",
    "exibir_imagem_com_explicacao(df_instancia, instancia_index, explicacao)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
