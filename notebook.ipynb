{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"\\n\\nSolving model....\\n\")\\n\\nmsol = mdl.solve(log_output=True)\\nprint(mdl.get_solve_status())\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docplex.mp.model as mp\n",
    "from cplex import infinity\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def codify_network_fischetti(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "    bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(\n",
    "                    A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                )\n",
    "                mdl.add_indicator(a[j], y[j] <= 0, 1)\n",
    "                mdl.add_indicator(a[j], s[j] <= 0, 0)\n",
    "\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub_y = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.maximize(s[j])\n",
    "                mdl.solve()\n",
    "                ub_s = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub_y)\n",
    "                s[j].set_ub(ub_s)\n",
    "\n",
    "                bounds.append([-ub_s, ub_y])\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j], ctname=f\"c_{i}_{j}\")\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.minimize(y[j])\n",
    "                mdl.solve()\n",
    "                lb = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub)\n",
    "                y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "                \n",
    "                bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds, bounds\n",
    "\n",
    "\n",
    "# todo: ver se faz uma chamada para cada classe não predita\n",
    "def codify_network_fischetti_relaxed(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds = []\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):  # para cada camada\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]): # para cada neuronio da camada\n",
    "            if i != len(layers) - 1:  # se não for a última camada(camada de saída)\n",
    "                m_less, m_more = bounds[j]\n",
    "                if m_more <= 0:\n",
    "                    mdl.add_constraint(y[j] == 0)\n",
    "                    continue\n",
    "\n",
    "                if m_less >= 0:\n",
    "                    mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                    continue\n",
    "\n",
    "                if m_less < 0 and m_more > 0:\n",
    "                    mdl.add_constraint(\n",
    "                        A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                    )\n",
    "                    mdl.add_constraint(y[j] <= m_more * (1 - a[j]))\n",
    "                    mdl.add_constraint(s[j] <= -m_less * a[j])\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                lb, ub = output_bounds_binary_variables[j]\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "def codify_network_tjeng(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            mdl.maximize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            ub = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if ub <= 0 and i != len(layers) - 1:\n",
    "                print(\"ENTROU, o ub é negativo, logo y = 0\")\n",
    "                mdl.add_constraint(y[j] == 0, ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            mdl.minimize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            lb = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if lb >= 0 and i != len(layers) - 1:\n",
    "                print(\"ENTROU, o lb >= 0, logo y = Wx + b\")\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j], ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(y[j] <= A[j, :] @ x + b[j] - lb * (1 - a[j]))\n",
    "                mdl.add_constraint(y[j] >= A[j, :] @ x + b[j])\n",
    "                mdl.add_constraint(y[j] <= ub * a[j])\n",
    "\n",
    "                # mdl.maximize(y[j])\n",
    "                # mdl.solve()\n",
    "                # ub_y = mdl.solution.get_objective_value()\n",
    "                # mdl.remove_objective()\n",
    "                # y[j].set_ub(ub_y)\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                # y[j].set_ub(ub)\n",
    "                # y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "def codify_network(model, dataframe, method, relaxe_constraints):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:, 0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            decision_variables.append(\n",
    "                mdl.binary_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        mdl, output_bounds, bounds = codify_network_fischetti(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                a.set_vartype(\"Integer\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds, bounds\n",
    "\n",
    "\n",
    "def get_domain_and_bounds_inputs(dataframe):\n",
    "    domain = []\n",
    "    bounds = []\n",
    "    for column in dataframe.columns[:-1]:\n",
    "        if len(dataframe[column].unique()) == 2:\n",
    "            domain.append(\"B\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        elif np.any(\n",
    "            dataframe[column].unique().astype(np.int64)\n",
    "            != dataframe[column].unique().astype(np.float64)\n",
    "        ):\n",
    "            domain.append(\"C\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        else:\n",
    "            domain.append(\"I\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "\n",
    "    return domain, bounds\n",
    "\n",
    "\n",
    "def codify_network_relaxed(\n",
    "    model, dataframe, method, relaxe_constraints, output_bounds_binary_variables, bounds\n",
    "):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:, 0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # decision_variables.append(mdl.binary_var_list(weights.shape[1], name='a', lb=0, ub=1, key_format=f\"_{i}_%s\"))\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        # modificar depois para utilizar bounds precisos\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        # mdl, output_bounds = codify_network_fischetti(mdl, layers, input_variables, auxiliary_variables, intermediate_variables, decision_variables, output_variables)\n",
    "        mdl, output_bounds = codify_network_fischetti_relaxed(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "            output_bounds_binary_variables,\n",
    "            bounds = bounds\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                # a.set_vartype('Integer')\n",
    "                a.set_vartype(\"Continuous\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     path_dir = 'glass'\n",
    "#     #model = tf.keras.models.load_model(f'datasets\\\\{path_dir}\\\\model_{path_dir}.h5')\n",
    "#     model = tf.keras.models.load_model(f'datasets\\\\{path_dir}\\\\teste.h5')\n",
    "\n",
    "#     data_test = pd.read_csv(f'datasets\\\\{path_dir}\\\\test.csv')\n",
    "#     data_train = pd.read_csv(f'datasets\\\\{path_dir}\\\\train.csv')\n",
    "#     data = data_train._append(data_test)\n",
    "#     data = data[['RI', 'Na', 'target']]\n",
    "\n",
    "#     mdl, bounds = codify_network(model, data, 'tjeng', False)\n",
    "#     print(mdl.export_to_string())\n",
    "#     # print(bounds)\n",
    "#     print(\"Bounds:\")\n",
    "#     for bound in bounds:\n",
    "#         print(bound)\n",
    "\n",
    "# X ---- E\n",
    "# x1 == 1 /\\ x2 == 3 /\\ F /\\ ~E    INSATISFÁTIVEL\n",
    "# x1 >= 0 /\\ x1 <= 100 /\\ x2 == 3 /\\ F /\\ ~E    INSATISFÁTIVEL -> x1 n é relevante,  SATISFÁTIVEL -> x1 é relevante\n",
    "\"\"\"\n",
    "print(\"\\n\\nSolving model....\\n\")\n",
    "\n",
    "msol = mdl.solve(log_output=True)\n",
    "print(mdl.get_solve_status())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from milp import codify_network\n",
    "from time import time\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "from docplex.mp.constr import LinearConstraint\n",
    "\n",
    "# todo: ver se faz uma chamada para cada classe não predita\n",
    "def insert_output_constraints_fischetti(\n",
    "    mdl, output_variables, network_output, binary_variables\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            p = binary_variables[aux_var]\n",
    "            aux_var += 1\n",
    "            mdl.add_indicator(p, variable_output <= output, 1)\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def insert_output_constraints_tjeng(\n",
    "    mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    upper_bounds_diffs = (\n",
    "        output_bounds[network_output][1] - np.array(output_bounds)[:, 0]\n",
    "    )  # Output i: oi - oj <= u1 = ui - lj\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            ub = upper_bounds_diffs[i]\n",
    "            z = binary_variables[aux_var]\n",
    "            mdl.add_constraint(variable_output - output - ub * (1 - z) <= 0)\n",
    "            aux_var += 1\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def get_minimal_explanation(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    ") -> List[LinearConstraint]:\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\") # todo: como isso é utilizado dentro do insert_output_constraints_fischetti?\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None: \n",
    "            mdl.add_constraint(constraint)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n",
    "\n",
    "\n",
    "def get_explanation_relaxed(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    "    delta=0.1,\n",
    ") -> List[LinearConstraint]:\n",
    "    # todo: output_bounds só é relevante se o metodo for tjeng\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\")\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "\n",
    "    # todo: !(o1>o2 and o1>o3)\n",
    "    # todo: modificar para o1<=o2 or o1<=o3\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        x = constraint.get_left_expr()\n",
    "        v = constraint.get_right_expr()\n",
    "\n",
    "        constraint_left = mdl.add_constraint(v - delta <= x)\n",
    "        constraint_right = mdl.add_constraint(x <= v + delta)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None:\n",
    "            mdl.add_constraint(constraint)\n",
    "            mdl.remove_constraint(constraint_left)\n",
    "            mdl.remove_constraint(constraint_right)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    datasets = [  # {'dir_path': 'australian', 'n_classes': 2},\n",
    "        # {'dir_path': 'auto', 'n_classes': 5},\n",
    "        # {'dir_path': 'backache', 'n_classes': 2},\n",
    "        # {'dir_path': 'breast-cancer', 'n_classes': 2},\n",
    "        # {'dir_path': 'cleve', 'n_cla\n",
    "        # sses': 2},\n",
    "        # {'dir_path': 'cleveland', 'n_classes': 5},\n",
    "        # {'dir_path': 'glass', 'n_classes': 5},\n",
    "        {\"dir_path\": \"glass2\", \"n_classes\": 2},\n",
    "        # {'dir_path': 'heart-statlog', 'n_classes': 2}, {'dir_path': 'hepatitis', 'n_classes': 2},\n",
    "        # {'dir_path': 'spect', 'n_classes': 2},\n",
    "        # {'dir_path': 'voting', 'n_classes': 2}\n",
    "    ]\n",
    "\n",
    "    configurations = [  # {'method': 'fischetti', 'relaxe_constraints': True},\n",
    "        {\"method\": \"fischetti\", \"relaxe_constraints\": True},\n",
    "        # {'method': 'tjeng', 'relaxe_constraints': True},\n",
    "        {\"method\": \"tjeng\", \"relaxe_constraints\": False},\n",
    "    ]\n",
    "\n",
    "    df = {\n",
    "        \"fischetti\": {\n",
    "            True: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "            False: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "        },\n",
    "        \"tjeng\": {\n",
    "            True: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "            False: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dir_path = dataset[\"dir_path\"]\n",
    "        n_classes = dataset[\"n_classes\"]\n",
    "\n",
    "        for config in configurations:\n",
    "            print(dataset, config)\n",
    "\n",
    "            method = config[\"method\"]\n",
    "            relaxe_constraints = config[\"relaxe_constraints\"]\n",
    "\n",
    "            data_test = pd.read_csv(f\"datasets\\\\{dir_path}\\\\test.csv\")\n",
    "            data_train = pd.read_csv(f\"datasets\\\\{dir_path}\\\\train.csv\")\n",
    "            data = data_train._append(data_test)\n",
    "\n",
    "            model_path = f\"datasets\\\\{dir_path}\\\\model_4layers_{dir_path}.h5\"\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            codify_network_time = []\n",
    "            for _ in range(10):\n",
    "                start = time()\n",
    "                mdl, output_bounds = codify_network(\n",
    "                    model, data, method, relaxe_constraints\n",
    "                )\n",
    "                codify_network_time.append(time() - start)\n",
    "                print(codify_network_time[-1])\n",
    "\n",
    "            time_list = []\n",
    "            len_list = []\n",
    "            # data = data.to_numpy()\n",
    "            data = data_test.to_numpy()\n",
    "            for i in range(data.shape[0]):\n",
    "                # if i % 50 == 0:\n",
    "                print(i)\n",
    "                network_input = data[i, :-1]\n",
    "\n",
    "                network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "                network_output = model.predict(tf.constant(network_input))[0]\n",
    "                network_output = tf.argmax(network_output)\n",
    "\n",
    "                mdl_aux = mdl.clone()\n",
    "                start = time()\n",
    "\n",
    "                explanation = get_minimal_explanation(\n",
    "                    mdl_aux,\n",
    "                    network_input,\n",
    "                    network_output,\n",
    "                    n_classes=n_classes,\n",
    "                    method=method,\n",
    "                    output_bounds=output_bounds,\n",
    "                )\n",
    "\n",
    "                time_list.append(time() - start)\n",
    "\n",
    "                len_list.append(len(explanation))\n",
    "\n",
    "            df[method][relaxe_constraints][\"size\"].extend(\n",
    "                [min(len_list), f\"{mean(len_list)} +- {stdev(len_list)}\", max(len_list)]\n",
    "            )\n",
    "            df[method][relaxe_constraints][\"milp_time\"].extend(\n",
    "                [\n",
    "                    min(time_list),\n",
    "                    f\"{mean(time_list)} +- {stdev(time_list)}\",\n",
    "                    max(time_list),\n",
    "                ]\n",
    "            )\n",
    "            df[method][relaxe_constraints][\"build_time\"].extend(\n",
    "                [\n",
    "                    min(codify_network_time),\n",
    "                    f\"{mean(codify_network_time)} +- {stdev(codify_network_time)}\",\n",
    "                    max(codify_network_time),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Explication sizes:\\nm: {min(len_list)}\\na: {mean(len_list)} +- {stdev(len_list)}\\nM: {max(len_list)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Time:\\nm: {min(time_list)}\\na: {mean(time_list)} +- {stdev(time_list)}\\nM: {max(time_list)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Build Time:\\nm: {min(codify_network_time)}\\na: {mean(codify_network_time)} +- {stdev(codify_network_time)}\\nM: {max(codify_network_time)}\"\n",
    "            )\n",
    "    \"a\" + 1\n",
    "    df = {\n",
    "        \"fischetti_relaxe_size\": df[\"fischetti\"][True][\"size\"],\n",
    "        \"fischetti_relaxe_time\": df[\"fischetti\"][True][\"milp_time\"],\n",
    "        \"fischetti_relaxe_build_time\": df[\"fischetti\"][True][\"build_time\"],\n",
    "        \"fischetti_not_relaxe_size\": df[\"fischetti\"][False][\"size\"],\n",
    "        \"fischetti_not_relaxe_time\": df[\"fischetti\"][False][\"milp_time\"],\n",
    "        \"fischetti_not_relaxe_build_time\": df[\"fischetti\"][False][\"build_time\"],\n",
    "        \"tjeng_relaxe_size\": df[\"tjeng\"][True][\"size\"],\n",
    "        \"tjeng_relaxe_time\": df[\"tjeng\"][True][\"milp_time\"],\n",
    "        \"tjeng_relaxe_build_time\": df[\"tjeng\"][True][\"build_time\"],\n",
    "        \"tjeng_not_relaxe_size\": df[\"tjeng\"][False][\"size\"],\n",
    "        \"tjeng_not_relaxe_time\": df[\"tjeng\"][False][\"milp_time\"],\n",
    "        \"tjeng_not_relaxe_build_time\": df[\"tjeng\"][False][\"build_time\"],\n",
    "    }\n",
    "\n",
    "    index_label = []\n",
    "    for dataset in datasets:\n",
    "        index_label.extend(\n",
    "            [\n",
    "                f\"{dataset['dir_path']}_m\",\n",
    "                f\"{dataset['dir_path']}_a\",\n",
    "                f\"{dataset['dir_path']}_M\",\n",
    "            ]\n",
    "        )\n",
    "    df = pd.DataFrame(data=df, index=index_label)\n",
    "    df.to_csv(\"results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from milp import codify_network, codify_network_relaxed\n",
    "from teste import get_explanation_relaxed, get_minimal_explanation\n",
    "from typing import List\n",
    "from docplex.mp.constr import LinearConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede(dir_path: str, num_classes: int, n_neurons: int, n_hidden_layers: int):\n",
    "    data_train = pd.read_csv(dir_path + \"\\\\\" + \"train.csv\").to_numpy()\n",
    "    data_test = pd.read_csv(dir_path + \"\\\\\" + \"test.csv\").to_numpy()\n",
    "\n",
    "    x_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "    x_test, y_test = data_test[:, :-1], data_test[:, -1]\n",
    "\n",
    "    y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=[x_train.shape[1]]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        dir_path, \"models\", f\"model_{n_hidden_layers}layers_{n_neurons}neurons.h5\"\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    ck = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor=\"val_accuracy\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train_ohe,\n",
    "        batch_size=4,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test_ohe),\n",
    "        verbose=2,\n",
    "        callbacks=[ck, es],\n",
    "    )\n",
    "    print(f\"Tempo de Treinamento: {time()-start}\")\n",
    "\n",
    "    # salvar modelo\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # avaliar modelo com os dados de treinamento\n",
    "    print(\"Resultado Treinamento\")\n",
    "    model.evaluate(x_train, y_train_ohe, verbose=2)\n",
    "\n",
    "    # avaliar modelo com os dados de teste\n",
    "    print(\"Resultado Teste\")\n",
    "    model.evaluate(x_test, y_test_ohe, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede_com_dataset_iris(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\iris\"\n",
    "    num_classes = 3\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_digits(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\digits\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_wine(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\wine\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(initial_network,\n",
    "    dataset: {}, configuration: {}, instance_index: int\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "    \n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = initial_network\n",
    "        \n",
    "    \n",
    "    # todo: ao inves de receber um, receber varios a serem explicados sem ter que codificar uma nova rede\n",
    "    # todo: salvar a rede de alguma forma para reutilizar\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "    mdl_aux = mdl_milp_with_binary_variable.clone()\n",
    "\n",
    "    explanation = get_minimal_explanation(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables,\n",
    "    )\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance_relaxed(\n",
    "    initial_network,\n",
    "    initial_network_relaxed,\n",
    "    dataset: {},\n",
    "    configuration: {},\n",
    "    instance_index: int,\n",
    "    delta=1,\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "    \n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = initial_network\n",
    "    \n",
    "    model_milp_relaxed, output_bounds_relaxed = initial_network_relaxed\n",
    "\n",
    "\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "\n",
    "    mdl_aux = model_milp_relaxed.clone()\n",
    "\n",
    "    explanation = get_explanation_relaxed(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables,  # output_bounds_binary_variables == output_bounds_relaxed\n",
    "        delta=delta,\n",
    "    )\n",
    "\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explicar_rede():\n",
    "#     datasets = [\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/digits\",\n",
    "#             \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "#             \"n_classes\": 10,\n",
    "#         },\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/iris\",\n",
    "#             \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "#             \"n_classes\": 3,\n",
    "#         },\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/iris\",\n",
    "#             \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "#             \"n_classes\": 3,\n",
    "#         },\n",
    "#     ]\n",
    "#     configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n",
    "    \n",
    "#     dataset_index = 0\n",
    "\n",
    "#     for i in range(0, 1):\n",
    "#         print(\"binaria\", end = '')\n",
    "#         explanation = explain_instance(\n",
    "#             dataset=datasets[dataset_index], configuration=configurations[0], instance_index=i\n",
    "#         )\n",
    "\n",
    "#         # for x in explanation:\n",
    "#         #     print(x)\n",
    "#         print(\"len: \", len(explanation), \"\\n\")\n",
    "\n",
    "        \n",
    "#         print(\"relaxada\", end = '')\n",
    "#         explanation = explain_instance_relaxed(\n",
    "#             dataset=datasets[dataset_index], configuration=configurations[0], instance_index=i, delta = 0.5\n",
    "#         )\n",
    "\n",
    "#         # for x in explanation:\n",
    "#         #     print(x)\n",
    "#         print(\"len: \", len(explanation), \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicar_rede()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "        {\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_2layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },{\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_3layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },{\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_4layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },{\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_5layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "    ]\n",
    "configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificar Redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dataset_index = 3\n",
    "\n",
    "dir_path, n_classes, model = (\n",
    "    datasets[dataset_index][\"dir_path\"],\n",
    "    datasets[dataset_index][\"n_classes\"],\n",
    "    datasets[dataset_index][\"model\"],\n",
    ")\n",
    "\n",
    "method = configurations[0][\"method\"]\n",
    "relaxe_constraints = configurations[0][\"relaxe_constraints\"]\n",
    "data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "data = data_train._append(data_test)\n",
    "model = tf.keras.models.load_model(f\"{dir_path}/{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_network = codify_network(model, data, method, relaxe_constraints)\n",
    "(\n",
    "    mdl_milp_with_binary_variable,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds,\n",
    ") = initial_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_network_relaxed = codify_network_relaxed(\n",
    "    model,\n",
    "    data,\n",
    "    method,\n",
    "    relaxe_constraints,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds=bounds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "caminho_do_diretorio = \"C:\\\\Users\\\\mylle\\\\OneDrive\\\\Documentos\\\\GitHub\\\\TestesTypescript\\\\Explications-ANNs\"\n",
    "nome_do_arquivo = f\"{datasets[dataset_index]['dir_path']}/{datasets[dataset_index]['model']}_resultados.csv\"\n",
    "caminho_completo = f\"{caminho_do_diretorio}/{nome_do_arquivo}\"\n",
    "\n",
    "if os.path.isfile(caminho_completo):\n",
    "    # Se existe, leia o CSV\n",
    "    resultados = pd.read_csv(caminho_completo)\n",
    "else:\n",
    "    # Se não existe, crie um DataFrame vazio\n",
    "    resultados = pd.DataFrame(columns=[\"instance_index\", \"tempo_original\", \"tempo_relaxado\", \"len_original\", \"len_relaxado\", \"delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "10\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "11\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "12\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "13\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "14\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "15\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "16\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "17\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "18\n",
      "1/1 [==============================] - 0s 228ms/step\n"
     ]
    }
   ],
   "source": [
    "len_resultados = len(resultados)\n",
    "qtd = 10\n",
    "delta = 0.5\n",
    "\n",
    "def limpar_console():\n",
    "    os.system('clear')\n",
    "\n",
    "for instance_index in range(len_resultados, len_resultados+qtd):\n",
    "    print(instance_index)\n",
    "    # explain_instance\n",
    "    start_time = time.time()\n",
    "    explanation = explain_instance(\n",
    "        initial_network=initial_network,\n",
    "        dataset=datasets[dataset_index],\n",
    "        configuration=configurations[0],\n",
    "        instance_index=instance_index,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    tempo_original = end_time - start_time\n",
    "    len_original = len(explanation)\n",
    "\n",
    "    # explain_instance_relaxed\n",
    "    start_time = time.time()\n",
    "    explanation_relaxed = explain_instance_relaxed(\n",
    "        initial_network=initial_network,\n",
    "        initial_network_relaxed=initial_network_relaxed,\n",
    "        dataset=datasets[dataset_index],\n",
    "        configuration=configurations[0],\n",
    "        instance_index=instance_index,\n",
    "        delta=delta,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    tempo_relaxado = end_time - start_time\n",
    "    len_relaxado = len(explanation_relaxed)\n",
    "    resultados.loc[len(resultados)] = [\n",
    "        instance_index,\n",
    "        tempo_original,\n",
    "        tempo_relaxado,\n",
    "        len_original,\n",
    "        len_relaxado,\n",
    "        delta\n",
    "    ]\n",
    "    # salvar\n",
    "    resultados.to_csv(caminho_completo, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar o DataFrame como CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultados.to_csv(caminho_completo, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtrado = resultados[resultados['len_relaxado'] < 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(resultados))\n",
    "print(len(filtrado))\n",
    "len_r = len(resultados)\n",
    "len_f = len(filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_f*100/len_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
