{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"\\n\\nSolving model....\\n\")\\n\\nmsol = mdl.solve(log_output=True)\\nprint(mdl.get_solve_status())\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docplex.mp.model as mp\n",
    "from cplex import infinity\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def codify_network_fischetti(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "    bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(\n",
    "                    A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                )\n",
    "                mdl.add_indicator(a[j], y[j] <= 0, 1)\n",
    "                mdl.add_indicator(a[j], s[j] <= 0, 0)\n",
    "\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub_y = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.maximize(s[j])\n",
    "                mdl.solve()\n",
    "                ub_s = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub_y)\n",
    "                s[j].set_ub(ub_s)\n",
    "\n",
    "                bounds.append([-ub_s, ub_y])\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j], ctname=f\"c_{i}_{j}\")\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.minimize(y[j])\n",
    "                mdl.solve()\n",
    "                lb = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub)\n",
    "                y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "                \n",
    "                bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds, bounds\n",
    "\n",
    "\n",
    "# todo: ver se faz uma chamada para cada classe não predita\n",
    "def codify_network_fischetti_relaxed(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds = []\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):  # para cada camada\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]): # para cada neuronio da camada\n",
    "            if i != len(layers) - 1:  # se não for a última camada(camada de saída)\n",
    "                m_less, m_more = bounds[j]\n",
    "                if m_more <= 0:\n",
    "                    mdl.add_constraint(y[j] == 0)\n",
    "                    continue\n",
    "\n",
    "                if m_less >= 0:\n",
    "                    mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                    continue\n",
    "\n",
    "                if m_less < 0 and m_more > 0:\n",
    "                    mdl.add_constraint(\n",
    "                        A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                    )\n",
    "                    mdl.add_constraint(y[j] <= m_more * (1 - a[j]))\n",
    "                    mdl.add_constraint(s[j] <= -m_less * a[j])\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                lb, ub = output_bounds_binary_variables[j]\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "def codify_network_tjeng(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            mdl.maximize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            ub = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if ub <= 0 and i != len(layers) - 1:\n",
    "                print(\"ENTROU, o ub é negativo, logo y = 0\")\n",
    "                mdl.add_constraint(y[j] == 0, ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            mdl.minimize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            lb = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if lb >= 0 and i != len(layers) - 1:\n",
    "                print(\"ENTROU, o lb >= 0, logo y = Wx + b\")\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j], ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(y[j] <= A[j, :] @ x + b[j] - lb * (1 - a[j]))\n",
    "                mdl.add_constraint(y[j] >= A[j, :] @ x + b[j])\n",
    "                mdl.add_constraint(y[j] <= ub * a[j])\n",
    "\n",
    "                # mdl.maximize(y[j])\n",
    "                # mdl.solve()\n",
    "                # ub_y = mdl.solution.get_objective_value()\n",
    "                # mdl.remove_objective()\n",
    "                # y[j].set_ub(ub_y)\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                # y[j].set_ub(ub)\n",
    "                # y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "def codify_network(model, dataframe, method, relaxe_constraints):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:, 0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            decision_variables.append(\n",
    "                mdl.binary_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        mdl, output_bounds, bounds = codify_network_fischetti(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                a.set_vartype(\"Integer\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds, bounds\n",
    "\n",
    "\n",
    "def get_domain_and_bounds_inputs(dataframe):\n",
    "    domain = []\n",
    "    bounds = []\n",
    "    for column in dataframe.columns[:-1]:\n",
    "        if len(dataframe[column].unique()) == 2:\n",
    "            domain.append(\"B\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        elif np.any(\n",
    "            dataframe[column].unique().astype(np.int64)\n",
    "            != dataframe[column].unique().astype(np.float64)\n",
    "        ):\n",
    "            domain.append(\"C\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        else:\n",
    "            domain.append(\"I\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "\n",
    "    return domain, bounds\n",
    "\n",
    "\n",
    "def codify_network_relaxed(\n",
    "    model, dataframe, method, relaxe_constraints, output_bounds_binary_variables, bounds\n",
    "):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:, 0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # decision_variables.append(mdl.binary_var_list(weights.shape[1], name='a', lb=0, ub=1, key_format=f\"_{i}_%s\"))\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        # modificar depois para utilizar bounds precisos\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        # mdl, output_bounds = codify_network_fischetti(mdl, layers, input_variables, auxiliary_variables, intermediate_variables, decision_variables, output_variables)\n",
    "        mdl, output_bounds = codify_network_fischetti_relaxed(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "            output_bounds_binary_variables,\n",
    "            bounds = bounds\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                # a.set_vartype('Integer')\n",
    "                a.set_vartype(\"Continuous\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     path_dir = 'glass'\n",
    "#     #model = tf.keras.models.load_model(f'datasets\\\\{path_dir}\\\\model_{path_dir}.h5')\n",
    "#     model = tf.keras.models.load_model(f'datasets\\\\{path_dir}\\\\teste.h5')\n",
    "\n",
    "#     data_test = pd.read_csv(f'datasets\\\\{path_dir}\\\\test.csv')\n",
    "#     data_train = pd.read_csv(f'datasets\\\\{path_dir}\\\\train.csv')\n",
    "#     data = data_train._append(data_test)\n",
    "#     data = data[['RI', 'Na', 'target']]\n",
    "\n",
    "#     mdl, bounds = codify_network(model, data, 'tjeng', False)\n",
    "#     print(mdl.export_to_string())\n",
    "#     # print(bounds)\n",
    "#     print(\"Bounds:\")\n",
    "#     for bound in bounds:\n",
    "#         print(bound)\n",
    "\n",
    "# X ---- E\n",
    "# x1 == 1 /\\ x2 == 3 /\\ F /\\ ~E    INSATISFÁTIVEL\n",
    "# x1 >= 0 /\\ x1 <= 100 /\\ x2 == 3 /\\ F /\\ ~E    INSATISFÁTIVEL -> x1 n é relevante,  SATISFÁTIVEL -> x1 é relevante\n",
    "\"\"\"\n",
    "print(\"\\n\\nSolving model....\\n\")\n",
    "\n",
    "msol = mdl.solve(log_output=True)\n",
    "print(mdl.get_solve_status())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from milp import codify_network\n",
    "from time import time\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "from docplex.mp.constr import LinearConstraint\n",
    "\n",
    "# todo: ver se faz uma chamada para cada classe não predita\n",
    "def insert_output_constraints_fischetti(\n",
    "    mdl, output_variables, network_output, binary_variables\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            p = binary_variables[aux_var]\n",
    "            aux_var += 1\n",
    "            mdl.add_indicator(p, variable_output <= output, 1)\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def insert_output_constraints_tjeng(\n",
    "    mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    upper_bounds_diffs = (\n",
    "        output_bounds[network_output][1] - np.array(output_bounds)[:, 0]\n",
    "    )  # Output i: oi - oj <= u1 = ui - lj\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            ub = upper_bounds_diffs[i]\n",
    "            z = binary_variables[aux_var]\n",
    "            mdl.add_constraint(variable_output - output - ub * (1 - z) <= 0)\n",
    "            aux_var += 1\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def get_minimal_explanation(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    ") -> List[LinearConstraint]:\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\") # todo: como isso é utilizado dentro do insert_output_constraints_fischetti?\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None: \n",
    "            mdl.add_constraint(constraint)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n",
    "\n",
    "\n",
    "def get_explanation_relaxed(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    "    delta=0.1,\n",
    ") -> List[LinearConstraint]:\n",
    "    # todo: output_bounds só é relevante se o metodo for tjeng\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\")\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "\n",
    "    # todo: !(o1>o2 and o1>o3)\n",
    "    # todo: modificar para o1<=o2 or o1<=o3\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        x = constraint.get_left_expr()\n",
    "        v = constraint.get_right_expr()\n",
    "\n",
    "        constraint_left = mdl.add_constraint(v - delta <= x)\n",
    "        constraint_right = mdl.add_constraint(x <= v + delta)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None:\n",
    "            mdl.add_constraint(constraint)\n",
    "            mdl.remove_constraint(constraint_left)\n",
    "            mdl.remove_constraint(constraint_right)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    datasets = [  # {'dir_path': 'australian', 'n_classes': 2},\n",
    "        # {'dir_path': 'auto', 'n_classes': 5},\n",
    "        # {'dir_path': 'backache', 'n_classes': 2},\n",
    "        # {'dir_path': 'breast-cancer', 'n_classes': 2},\n",
    "        # {'dir_path': 'cleve', 'n_cla\n",
    "        # sses': 2},\n",
    "        # {'dir_path': 'cleveland', 'n_classes': 5},\n",
    "        # {'dir_path': 'glass', 'n_classes': 5},\n",
    "        {\"dir_path\": \"glass2\", \"n_classes\": 2},\n",
    "        # {'dir_path': 'heart-statlog', 'n_classes': 2}, {'dir_path': 'hepatitis', 'n_classes': 2},\n",
    "        # {'dir_path': 'spect', 'n_classes': 2},\n",
    "        # {'dir_path': 'voting', 'n_classes': 2}\n",
    "    ]\n",
    "\n",
    "    configurations = [  # {'method': 'fischetti', 'relaxe_constraints': True},\n",
    "        {\"method\": \"fischetti\", \"relaxe_constraints\": True},\n",
    "        # {'method': 'tjeng', 'relaxe_constraints': True},\n",
    "        {\"method\": \"tjeng\", \"relaxe_constraints\": False},\n",
    "    ]\n",
    "\n",
    "    df = {\n",
    "        \"fischetti\": {\n",
    "            True: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "            False: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "        },\n",
    "        \"tjeng\": {\n",
    "            True: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "            False: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dir_path = dataset[\"dir_path\"]\n",
    "        n_classes = dataset[\"n_classes\"]\n",
    "\n",
    "        for config in configurations:\n",
    "            print(dataset, config)\n",
    "\n",
    "            method = config[\"method\"]\n",
    "            relaxe_constraints = config[\"relaxe_constraints\"]\n",
    "\n",
    "            data_test = pd.read_csv(f\"datasets\\\\{dir_path}\\\\test.csv\")\n",
    "            data_train = pd.read_csv(f\"datasets\\\\{dir_path}\\\\train.csv\")\n",
    "            data = data_train._append(data_test)\n",
    "\n",
    "            model_path = f\"datasets\\\\{dir_path}\\\\model_4layers_{dir_path}.h5\"\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            codify_network_time = []\n",
    "            for _ in range(10):\n",
    "                start = time()\n",
    "                mdl, output_bounds = codify_network(\n",
    "                    model, data, method, relaxe_constraints\n",
    "                )\n",
    "                codify_network_time.append(time() - start)\n",
    "                print(codify_network_time[-1])\n",
    "\n",
    "            time_list = []\n",
    "            len_list = []\n",
    "            # data = data.to_numpy()\n",
    "            data = data_test.to_numpy()\n",
    "            for i in range(data.shape[0]):\n",
    "                # if i % 50 == 0:\n",
    "                print(i)\n",
    "                network_input = data[i, :-1]\n",
    "\n",
    "                network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "                network_output = model.predict(tf.constant(network_input))[0]\n",
    "                network_output = tf.argmax(network_output)\n",
    "\n",
    "                mdl_aux = mdl.clone()\n",
    "                start = time()\n",
    "\n",
    "                explanation = get_minimal_explanation(\n",
    "                    mdl_aux,\n",
    "                    network_input,\n",
    "                    network_output,\n",
    "                    n_classes=n_classes,\n",
    "                    method=method,\n",
    "                    output_bounds=output_bounds,\n",
    "                )\n",
    "\n",
    "                time_list.append(time() - start)\n",
    "\n",
    "                len_list.append(len(explanation))\n",
    "\n",
    "            df[method][relaxe_constraints][\"size\"].extend(\n",
    "                [min(len_list), f\"{mean(len_list)} +- {stdev(len_list)}\", max(len_list)]\n",
    "            )\n",
    "            df[method][relaxe_constraints][\"milp_time\"].extend(\n",
    "                [\n",
    "                    min(time_list),\n",
    "                    f\"{mean(time_list)} +- {stdev(time_list)}\",\n",
    "                    max(time_list),\n",
    "                ]\n",
    "            )\n",
    "            df[method][relaxe_constraints][\"build_time\"].extend(\n",
    "                [\n",
    "                    min(codify_network_time),\n",
    "                    f\"{mean(codify_network_time)} +- {stdev(codify_network_time)}\",\n",
    "                    max(codify_network_time),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Explication sizes:\\nm: {min(len_list)}\\na: {mean(len_list)} +- {stdev(len_list)}\\nM: {max(len_list)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Time:\\nm: {min(time_list)}\\na: {mean(time_list)} +- {stdev(time_list)}\\nM: {max(time_list)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Build Time:\\nm: {min(codify_network_time)}\\na: {mean(codify_network_time)} +- {stdev(codify_network_time)}\\nM: {max(codify_network_time)}\"\n",
    "            )\n",
    "    \"a\" + 1\n",
    "    df = {\n",
    "        \"fischetti_relaxe_size\": df[\"fischetti\"][True][\"size\"],\n",
    "        \"fischetti_relaxe_time\": df[\"fischetti\"][True][\"milp_time\"],\n",
    "        \"fischetti_relaxe_build_time\": df[\"fischetti\"][True][\"build_time\"],\n",
    "        \"fischetti_not_relaxe_size\": df[\"fischetti\"][False][\"size\"],\n",
    "        \"fischetti_not_relaxe_time\": df[\"fischetti\"][False][\"milp_time\"],\n",
    "        \"fischetti_not_relaxe_build_time\": df[\"fischetti\"][False][\"build_time\"],\n",
    "        \"tjeng_relaxe_size\": df[\"tjeng\"][True][\"size\"],\n",
    "        \"tjeng_relaxe_time\": df[\"tjeng\"][True][\"milp_time\"],\n",
    "        \"tjeng_relaxe_build_time\": df[\"tjeng\"][True][\"build_time\"],\n",
    "        \"tjeng_not_relaxe_size\": df[\"tjeng\"][False][\"size\"],\n",
    "        \"tjeng_not_relaxe_time\": df[\"tjeng\"][False][\"milp_time\"],\n",
    "        \"tjeng_not_relaxe_build_time\": df[\"tjeng\"][False][\"build_time\"],\n",
    "    }\n",
    "\n",
    "    index_label = []\n",
    "    for dataset in datasets:\n",
    "        index_label.extend(\n",
    "            [\n",
    "                f\"{dataset['dir_path']}_m\",\n",
    "                f\"{dataset['dir_path']}_a\",\n",
    "                f\"{dataset['dir_path']}_M\",\n",
    "            ]\n",
    "        )\n",
    "    df = pd.DataFrame(data=df, index=index_label)\n",
    "    df.to_csv(\"results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from milp import codify_network, codify_network_relaxed\n",
    "from teste import get_explanation_relaxed, get_minimal_explanation\n",
    "from typing import List\n",
    "from docplex.mp.constr import LinearConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede(dir_path: str, num_classes: int, n_neurons: int, n_hidden_layers: int):\n",
    "    data_train = pd.read_csv(dir_path + \"\\\\\" + \"train.csv\").to_numpy()\n",
    "    data_test = pd.read_csv(dir_path + \"\\\\\" + \"test.csv\").to_numpy()\n",
    "\n",
    "    x_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "    x_test, y_test = data_test[:, :-1], data_test[:, -1]\n",
    "\n",
    "    y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=[x_train.shape[1]]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        dir_path, \"models\", f\"model_{n_hidden_layers}layers_{n_neurons}neurons.h5\"\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    ck = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor=\"val_accuracy\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train_ohe,\n",
    "        batch_size=4,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test_ohe),\n",
    "        verbose=2,\n",
    "        callbacks=[ck, es],\n",
    "    )\n",
    "    print(f\"Tempo de Treinamento: {time()-start}\")\n",
    "\n",
    "    # salvar modelo\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # avaliar modelo com os dados de treinamento\n",
    "    print(\"Resultado Treinamento\")\n",
    "    model.evaluate(x_train, y_train_ohe, verbose=2)\n",
    "\n",
    "    # avaliar modelo com os dados de teste\n",
    "    print(\"Resultado Teste\")\n",
    "    model.evaluate(x_test, y_test_ohe, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede_com_dataset_iris(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\iris\"\n",
    "    num_classes = 3\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_digits(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\digits\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_wine(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\wine\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(\n",
    "    dataset: {}, configuration: {}, instance_index: int\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = codify_network(model, data, method, relaxe_constraints)\n",
    "    \n",
    "    # todo: ao inves de receber um, receber varios a serem explicados sem ter que codificar uma nova rede\n",
    "    # todo: salvar a rede de alguma forma para reutilizar\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "    mdl_aux = mdl_milp_with_binary_variable.clone()\n",
    "\n",
    "    explanation = get_minimal_explanation(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables,\n",
    "    )\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance_relaxed(\n",
    "    dataset: {}, configuration: {}, instance_index: int, delta = 1\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = codify_network(model, data, method, relaxe_constraints)\n",
    "\n",
    "    model_milp_relaxed, output_bounds_relaxed = codify_network_relaxed(\n",
    "        model,\n",
    "        data,\n",
    "        method,\n",
    "        relaxe_constraints,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "\n",
    "    mdl_aux = model_milp_relaxed.clone()\n",
    "\n",
    "    explanation = get_explanation_relaxed(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables, # output_bounds_binary_variables == output_bounds_relaxed\n",
    "        delta = delta\n",
    "    )\n",
    "\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explicar_rede():\n",
    "#     datasets = [\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/digits\",\n",
    "#             \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "#             \"n_classes\": 10,\n",
    "#         },\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/iris\",\n",
    "#             \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "#             \"n_classes\": 3,\n",
    "#         },\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/iris\",\n",
    "#             \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "#             \"n_classes\": 3,\n",
    "#         },\n",
    "#     ]\n",
    "#     configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n",
    "    \n",
    "#     dataset_index = 0\n",
    "\n",
    "#     for i in range(0, 1):\n",
    "#         print(\"binaria\", end = '')\n",
    "#         explanation = explain_instance(\n",
    "#             dataset=datasets[dataset_index], configuration=configurations[0], instance_index=i\n",
    "#         )\n",
    "\n",
    "#         # for x in explanation:\n",
    "#         #     print(x)\n",
    "#         print(\"len: \", len(explanation), \"\\n\")\n",
    "\n",
    "        \n",
    "#         print(\"relaxada\", end = '')\n",
    "#         explanation = explain_instance_relaxed(\n",
    "#             dataset=datasets[dataset_index], configuration=configurations[0], instance_index=i, delta = 0.5\n",
    "#         )\n",
    "\n",
    "#         # for x in explanation:\n",
    "#         #     print(x)\n",
    "#         print(\"len: \", len(explanation), \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicar_rede()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "        {\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "    ]\n",
    "configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 1s 822ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# instance_index = 0\n",
    "dataset_index = 0\n",
    "resultados = pd.DataFrame(columns=[\"instance_index\",\"tempo_original\", \"tempo_relaxado\", \"len_original\", \"len_relaxado\"])\n",
    "for instance_index in range(50):\n",
    "  # explain_instance\n",
    "  start_time = time.time()\n",
    "  explanation = explain_instance(dataset=datasets[dataset_index], configuration=configurations[0], instance_index=instance_index)\n",
    "  end_time = time.time()\n",
    "  tempo_original = end_time - start_time\n",
    "  len_original = len(explanation)\n",
    "  # resultados.loc[len(resultados)] = [\"original\", tempo_execucao, comprimento_explicacao]\n",
    "  # explain_instance_relaxed\n",
    "  start_time = time.time()\n",
    "  explanation_relaxed = explain_instance_relaxed(dataset=datasets[dataset_index], configuration=configurations[0], instance_index=instance_index, delta=1)\n",
    "  end_time = time.time()\n",
    "  tempo_relaxado = end_time - start_time\n",
    "  len_relaxado = len(explanation_relaxed)\n",
    "  resultados.loc[len(resultados)] = [instance_index, tempo_original, tempo_relaxado, len_original, len_relaxado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_index</th>\n",
       "      <th>tempo_original</th>\n",
       "      <th>tempo_relaxado</th>\n",
       "      <th>len_original</th>\n",
       "      <th>len_relaxado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.327124</td>\n",
       "      <td>6.807495</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.374623</td>\n",
       "      <td>5.684054</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.342024</td>\n",
       "      <td>5.294648</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.206375</td>\n",
       "      <td>5.387155</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.839998</td>\n",
       "      <td>5.199343</td>\n",
       "      <td>46.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.558484</td>\n",
       "      <td>5.153048</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.293896</td>\n",
       "      <td>5.688077</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.661423</td>\n",
       "      <td>6.184143</td>\n",
       "      <td>40.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.531771</td>\n",
       "      <td>5.062894</td>\n",
       "      <td>40.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>6.720854</td>\n",
       "      <td>4.882955</td>\n",
       "      <td>47.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.632430</td>\n",
       "      <td>5.783116</td>\n",
       "      <td>40.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>7.311785</td>\n",
       "      <td>5.095775</td>\n",
       "      <td>50.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>7.146513</td>\n",
       "      <td>5.306575</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6.690066</td>\n",
       "      <td>4.585272</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>6.980627</td>\n",
       "      <td>4.701941</td>\n",
       "      <td>46.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.844542</td>\n",
       "      <td>4.490144</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>6.543286</td>\n",
       "      <td>4.994763</td>\n",
       "      <td>46.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>6.568114</td>\n",
       "      <td>4.784277</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6.547968</td>\n",
       "      <td>4.846793</td>\n",
       "      <td>47.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6.772887</td>\n",
       "      <td>4.384585</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.132950</td>\n",
       "      <td>5.180034</td>\n",
       "      <td>45.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7.287815</td>\n",
       "      <td>4.728750</td>\n",
       "      <td>41.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.575482</td>\n",
       "      <td>4.612404</td>\n",
       "      <td>40.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>6.048450</td>\n",
       "      <td>5.566132</td>\n",
       "      <td>41.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.0</td>\n",
       "      <td>6.423257</td>\n",
       "      <td>4.758741</td>\n",
       "      <td>46.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>5.945007</td>\n",
       "      <td>5.061157</td>\n",
       "      <td>47.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>7.444579</td>\n",
       "      <td>5.593578</td>\n",
       "      <td>48.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>6.742358</td>\n",
       "      <td>5.467260</td>\n",
       "      <td>50.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.0</td>\n",
       "      <td>6.473677</td>\n",
       "      <td>4.452819</td>\n",
       "      <td>47.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>6.738686</td>\n",
       "      <td>4.966004</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.0</td>\n",
       "      <td>6.200578</td>\n",
       "      <td>5.248167</td>\n",
       "      <td>41.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.0</td>\n",
       "      <td>6.483127</td>\n",
       "      <td>4.668043</td>\n",
       "      <td>48.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32.0</td>\n",
       "      <td>6.329510</td>\n",
       "      <td>5.519907</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33.0</td>\n",
       "      <td>7.207514</td>\n",
       "      <td>4.618820</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.0</td>\n",
       "      <td>6.237435</td>\n",
       "      <td>4.883666</td>\n",
       "      <td>45.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35.0</td>\n",
       "      <td>7.555997</td>\n",
       "      <td>4.763422</td>\n",
       "      <td>41.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36.0</td>\n",
       "      <td>7.268781</td>\n",
       "      <td>4.554811</td>\n",
       "      <td>41.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37.0</td>\n",
       "      <td>6.803222</td>\n",
       "      <td>4.748769</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.0</td>\n",
       "      <td>6.156989</td>\n",
       "      <td>5.196175</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39.0</td>\n",
       "      <td>7.302130</td>\n",
       "      <td>5.205530</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.0</td>\n",
       "      <td>6.954472</td>\n",
       "      <td>4.612667</td>\n",
       "      <td>39.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41.0</td>\n",
       "      <td>6.879680</td>\n",
       "      <td>4.971210</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42.0</td>\n",
       "      <td>8.103700</td>\n",
       "      <td>5.117260</td>\n",
       "      <td>40.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43.0</td>\n",
       "      <td>7.163278</td>\n",
       "      <td>5.036725</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44.0</td>\n",
       "      <td>6.928957</td>\n",
       "      <td>5.376776</td>\n",
       "      <td>43.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.160446</td>\n",
       "      <td>5.229526</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46.0</td>\n",
       "      <td>6.603851</td>\n",
       "      <td>5.149607</td>\n",
       "      <td>37.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47.0</td>\n",
       "      <td>5.794807</td>\n",
       "      <td>4.836740</td>\n",
       "      <td>47.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48.0</td>\n",
       "      <td>6.668784</td>\n",
       "      <td>4.776398</td>\n",
       "      <td>49.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49.0</td>\n",
       "      <td>6.847048</td>\n",
       "      <td>5.373820</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    instance_index  tempo_original  tempo_relaxado  len_original  len_relaxado\n",
       "0              0.0        8.327124        6.807495          43.0          64.0\n",
       "1              1.0        8.374623        5.684054          43.0          64.0\n",
       "2              2.0        6.342024        5.294648          44.0          64.0\n",
       "3              3.0        7.206375        5.387155          44.0          64.0\n",
       "4              4.0        7.839998        5.199343          46.0          64.0\n",
       "5              5.0        6.558484        5.153048          44.0          64.0\n",
       "6              6.0        7.293896        5.688077          44.0          64.0\n",
       "7              7.0        7.661423        6.184143          40.0          64.0\n",
       "8              8.0        6.531771        5.062894          40.0          64.0\n",
       "9              9.0        6.720854        4.882955          47.0          64.0\n",
       "10            10.0        7.632430        5.783116          40.0          64.0\n",
       "11            11.0        7.311785        5.095775          50.0          64.0\n",
       "12            12.0        7.146513        5.306575          43.0          64.0\n",
       "13            13.0        6.690066        4.585272          44.0          64.0\n",
       "14            14.0        6.980627        4.701941          46.0          64.0\n",
       "15            15.0        6.844542        4.490144          42.0          64.0\n",
       "16            16.0        6.543286        4.994763          46.0          64.0\n",
       "17            17.0        6.568114        4.784277          44.0          64.0\n",
       "18            18.0        6.547968        4.846793          47.0          64.0\n",
       "19            19.0        6.772887        4.384585          42.0          64.0\n",
       "20            20.0        7.132950        5.180034          45.0          64.0\n",
       "21            21.0        7.287815        4.728750          41.0          64.0\n",
       "22            22.0        7.575482        4.612404          40.0          64.0\n",
       "23            23.0        6.048450        5.566132          41.0          64.0\n",
       "24            24.0        6.423257        4.758741          46.0          64.0\n",
       "25            25.0        5.945007        5.061157          47.0          64.0\n",
       "26            26.0        7.444579        5.593578          48.0          64.0\n",
       "27            27.0        6.742358        5.467260          50.0          64.0\n",
       "28            28.0        6.473677        4.452819          47.0          64.0\n",
       "29            29.0        6.738686        4.966004          43.0          64.0\n",
       "30            30.0        6.200578        5.248167          41.0          64.0\n",
       "31            31.0        6.483127        4.668043          48.0          64.0\n",
       "32            32.0        6.329510        5.519907          49.0          64.0\n",
       "33            33.0        7.207514        4.618820          44.0          64.0\n",
       "34            34.0        6.237435        4.883666          45.0          64.0\n",
       "35            35.0        7.555997        4.763422          41.0          64.0\n",
       "36            36.0        7.268781        4.554811          41.0          64.0\n",
       "37            37.0        6.803222        4.748769          43.0          64.0\n",
       "38            38.0        6.156989        5.196175          49.0          64.0\n",
       "39            39.0        7.302130        5.205530          42.0          64.0\n",
       "40            40.0        6.954472        4.612667          39.0          64.0\n",
       "41            41.0        6.879680        4.971210          44.0          64.0\n",
       "42            42.0        8.103700        5.117260          40.0          64.0\n",
       "43            43.0        7.163278        5.036725          42.0          64.0\n",
       "44            44.0        6.928957        5.376776          43.0          64.0\n",
       "45            45.0        6.160446        5.229526          49.0          64.0\n",
       "46            46.0        6.603851        5.149607          37.0          64.0\n",
       "47            47.0        5.794807        4.836740          47.0          64.0\n",
       "48            48.0        6.668784        4.776398          49.0          64.0\n",
       "49            49.0        6.847048        5.373820          44.0          64.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_do_arquivo = \"dataset_resultados.csv\"\n",
    "# caminho_do_diretorio = \"/caminho/do/seu/diretorio\"\n",
    "caminho_do_diretorio = \"C:\\\\Users\\\\mylle\\\\OneDrive\\\\Documentos\\\\GitHub\\\\TestesTypescript\\\\Explications-ANNs\"\n",
    "caminho_completo = f\"{caminho_do_diretorio}/{nome_do_arquivo}\"\n",
    "\n",
    "# Salve o DataFrame como CSV\n",
    "resultados.to_csv(caminho_completo, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
