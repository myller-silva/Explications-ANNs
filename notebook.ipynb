{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"\\n\\nSolving model....\\n\")\\n\\nmsol = mdl.solve(log_output=True)\\nprint(mdl.get_solve_status())\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docplex.mp.model as mp\n",
    "from cplex import infinity\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def codify_network_fischetti(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "    bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(\n",
    "                    A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                )\n",
    "                mdl.add_indicator(a[j], y[j] <= 0, 1)\n",
    "                mdl.add_indicator(a[j], s[j] <= 0, 0)\n",
    "\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub_y = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.maximize(s[j])\n",
    "                mdl.solve()\n",
    "                ub_s = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub_y)\n",
    "                s[j].set_ub(ub_s)\n",
    "\n",
    "                bounds.append([-ub_s, ub_y])\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j], ctname=f\"c_{i}_{j}\")\n",
    "                mdl.maximize(y[j])\n",
    "                mdl.solve()\n",
    "                ub = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                mdl.minimize(y[j])\n",
    "                mdl.solve()\n",
    "                lb = mdl.solution.get_objective_value()\n",
    "                mdl.remove_objective()\n",
    "\n",
    "                y[j].set_ub(ub)\n",
    "                y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "                \n",
    "                bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds, bounds\n",
    "\n",
    "\n",
    "# todo: ver se faz uma chamada para cada classe não predita\n",
    "def codify_network_fischetti_relaxed(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    auxiliary_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "    output_bounds_binary_variables,\n",
    "    bounds = []\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):  # para cada camada\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            s = auxiliary_variables[i]\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]): # para cada neuronio da camada\n",
    "            if i != len(layers) - 1:  # se não for a última camada(camada de saída)\n",
    "                m_less, m_more = bounds[j]\n",
    "                if m_more <= 0:\n",
    "                    mdl.add_constraint(y[j] == 0)\n",
    "                    continue\n",
    "\n",
    "                if m_less >= 0:\n",
    "                    mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                    continue\n",
    "\n",
    "                if m_less < 0 and m_more > 0:\n",
    "                    mdl.add_constraint(\n",
    "                        A[j, :] @ x + b[j] == y[j] - s[j], ctname=f\"c_{i}_{j}\"\n",
    "                    )\n",
    "                    mdl.add_constraint(y[j] <= m_more * (1 - a[j]))\n",
    "                    mdl.add_constraint(s[j] <= -m_less * a[j])\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                lb, ub = output_bounds_binary_variables[j]\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "def codify_network_tjeng(\n",
    "    mdl,\n",
    "    layers,\n",
    "    input_variables,\n",
    "    intermediate_variables,\n",
    "    decision_variables,\n",
    "    output_variables,\n",
    "):\n",
    "    output_bounds = []\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        A = layers[i].get_weights()[0].T\n",
    "        b = layers[i].bias.numpy()\n",
    "        x = input_variables if i == 0 else intermediate_variables[i - 1]\n",
    "        if i != len(layers) - 1:\n",
    "            a = decision_variables[i]\n",
    "            y = intermediate_variables[i]\n",
    "        else:\n",
    "            y = output_variables\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            mdl.maximize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            ub = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if ub <= 0 and i != len(layers) - 1:\n",
    "                print(\"ENTROU, o ub é negativo, logo y = 0\")\n",
    "                mdl.add_constraint(y[j] == 0, ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            mdl.minimize(A[j, :] @ x + b[j])\n",
    "            mdl.solve()\n",
    "            lb = mdl.solution.get_objective_value()\n",
    "            mdl.remove_objective()\n",
    "\n",
    "            if lb >= 0 and i != len(layers) - 1:\n",
    "                print(\"ENTROU, o lb >= 0, logo y = Wx + b\")\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j], ctname=f\"c_{i}_{j}\")\n",
    "                continue\n",
    "\n",
    "            if i != len(layers) - 1:\n",
    "                mdl.add_constraint(y[j] <= A[j, :] @ x + b[j] - lb * (1 - a[j]))\n",
    "                mdl.add_constraint(y[j] >= A[j, :] @ x + b[j])\n",
    "                mdl.add_constraint(y[j] <= ub * a[j])\n",
    "\n",
    "                # mdl.maximize(y[j])\n",
    "                # mdl.solve()\n",
    "                # ub_y = mdl.solution.get_objective_value()\n",
    "                # mdl.remove_objective()\n",
    "                # y[j].set_ub(ub_y)\n",
    "\n",
    "            else:\n",
    "                mdl.add_constraint(A[j, :] @ x + b[j] == y[j])\n",
    "                # y[j].set_ub(ub)\n",
    "                # y[j].set_lb(lb)\n",
    "                output_bounds.append([lb, ub])\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "def codify_network(model, dataframe, method, relaxe_constraints):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:, 0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            decision_variables.append(\n",
    "                mdl.binary_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        mdl, output_bounds, bounds = codify_network_fischetti(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                a.set_vartype(\"Integer\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds, bounds\n",
    "\n",
    "\n",
    "def get_domain_and_bounds_inputs(dataframe):\n",
    "    domain = []\n",
    "    bounds = []\n",
    "    for column in dataframe.columns[:-1]:\n",
    "        if len(dataframe[column].unique()) == 2:\n",
    "            domain.append(\"B\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        elif np.any(\n",
    "            dataframe[column].unique().astype(np.int64)\n",
    "            != dataframe[column].unique().astype(np.float64)\n",
    "        ):\n",
    "            domain.append(\"C\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "        else:\n",
    "            domain.append(\"I\")\n",
    "            bound_inf = dataframe[column].min()\n",
    "            bound_sup = dataframe[column].max()\n",
    "            bounds.append([bound_inf, bound_sup])\n",
    "\n",
    "    return domain, bounds\n",
    "\n",
    "\n",
    "def codify_network_relaxed(\n",
    "    model, dataframe, method, relaxe_constraints, output_bounds_binary_variables, bounds\n",
    "):\n",
    "    layers = model.layers\n",
    "    num_features = layers[0].get_weights()[0].shape[0]\n",
    "    mdl = mp.Model()\n",
    "\n",
    "    domain_input, bounds_input = get_domain_and_bounds_inputs(dataframe)\n",
    "    bounds_input = np.array(bounds_input)\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        input_variables = mdl.continuous_var_list(\n",
    "            num_features, lb=bounds_input[:, 0], ub=bounds_input[:, 1], name=\"x\"\n",
    "        )\n",
    "    else:\n",
    "        input_variables = []\n",
    "        for i in range(len(domain_input)):\n",
    "            lb, ub = bounds_input[i]\n",
    "            if domain_input[i] == \"C\":\n",
    "                input_variables.append(mdl.continuous_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"I\":\n",
    "                input_variables.append(mdl.integer_var(lb=lb, ub=ub, name=f\"x_{i}\"))\n",
    "            elif domain_input[i] == \"B\":\n",
    "                input_variables.append(mdl.binary_var(name=f\"x_{i}\"))\n",
    "\n",
    "    intermediate_variables = []\n",
    "    auxiliary_variables = []\n",
    "    decision_variables = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = layers[i].get_weights()[0]\n",
    "        intermediate_variables.append(\n",
    "            mdl.continuous_var_list(\n",
    "                weights.shape[1], lb=0, name=\"y\", key_format=f\"_{i}_%s\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if method == \"fischetti\":\n",
    "            auxiliary_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], lb=0, name=\"s\", key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if relaxe_constraints and method == \"tjeng\":\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # decision_variables.append(mdl.binary_var_list(weights.shape[1], name='a', lb=0, ub=1, key_format=f\"_{i}_%s\"))\n",
    "            decision_variables.append(\n",
    "                mdl.continuous_var_list(\n",
    "                    weights.shape[1], name=\"a\", lb=0, ub=1, key_format=f\"_{i}_%s\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    output_variables = mdl.continuous_var_list(\n",
    "        layers[-1].get_weights()[0].shape[1], lb=-infinity, name=\"o\"\n",
    "    )\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        # modificar depois para utilizar bounds precisos\n",
    "        mdl, output_bounds = codify_network_tjeng(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "        )\n",
    "    else:\n",
    "        # mdl, output_bounds = codify_network_fischetti(mdl, layers, input_variables, auxiliary_variables, intermediate_variables, decision_variables, output_variables)\n",
    "        mdl, output_bounds = codify_network_fischetti_relaxed(\n",
    "            mdl,\n",
    "            layers,\n",
    "            input_variables,\n",
    "            auxiliary_variables,\n",
    "            intermediate_variables,\n",
    "            decision_variables,\n",
    "            output_variables,\n",
    "            output_bounds_binary_variables,\n",
    "            bounds = bounds\n",
    "        )\n",
    "\n",
    "    if relaxe_constraints:\n",
    "        # Tighten domain of variables 'a'\n",
    "        for i in decision_variables:\n",
    "            for a in i:\n",
    "                # a.set_vartype('Integer')\n",
    "                a.set_vartype(\"Continuous\")\n",
    "\n",
    "        # Tighten domain of input variables\n",
    "        for i, x in enumerate(input_variables):\n",
    "            if domain_input[i] == \"I\":\n",
    "                x.set_vartype(\"Integer\")\n",
    "            elif domain_input[i] == \"B\":\n",
    "                x.set_vartype(\"Binary\")\n",
    "            elif domain_input[i] == \"C\":\n",
    "                x.set_vartype(\"Continuous\")\n",
    "\n",
    "    return mdl, output_bounds\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     path_dir = 'glass'\n",
    "#     #model = tf.keras.models.load_model(f'datasets\\\\{path_dir}\\\\model_{path_dir}.h5')\n",
    "#     model = tf.keras.models.load_model(f'datasets\\\\{path_dir}\\\\teste.h5')\n",
    "\n",
    "#     data_test = pd.read_csv(f'datasets\\\\{path_dir}\\\\test.csv')\n",
    "#     data_train = pd.read_csv(f'datasets\\\\{path_dir}\\\\train.csv')\n",
    "#     data = data_train._append(data_test)\n",
    "#     data = data[['RI', 'Na', 'target']]\n",
    "\n",
    "#     mdl, bounds = codify_network(model, data, 'tjeng', False)\n",
    "#     print(mdl.export_to_string())\n",
    "#     # print(bounds)\n",
    "#     print(\"Bounds:\")\n",
    "#     for bound in bounds:\n",
    "#         print(bound)\n",
    "\n",
    "# X ---- E\n",
    "# x1 == 1 /\\ x2 == 3 /\\ F /\\ ~E    INSATISFÁTIVEL\n",
    "# x1 >= 0 /\\ x1 <= 100 /\\ x2 == 3 /\\ F /\\ ~E    INSATISFÁTIVEL -> x1 n é relevante,  SATISFÁTIVEL -> x1 é relevante\n",
    "\"\"\"\n",
    "print(\"\\n\\nSolving model....\\n\")\n",
    "\n",
    "msol = mdl.solve(log_output=True)\n",
    "print(mdl.get_solve_status())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from milp import codify_network\n",
    "from time import time\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "from docplex.mp.constr import LinearConstraint\n",
    "\n",
    "# todo: ver se faz uma chamada para cada classe não predita\n",
    "def insert_output_constraints_fischetti(\n",
    "    mdl, output_variables, network_output, binary_variables\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            p = binary_variables[aux_var]\n",
    "            aux_var += 1\n",
    "            mdl.add_indicator(p, variable_output <= output, 1)\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def insert_output_constraints_tjeng(\n",
    "    mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "):\n",
    "    variable_output = output_variables[network_output]\n",
    "    upper_bounds_diffs = (\n",
    "        output_bounds[network_output][1] - np.array(output_bounds)[:, 0]\n",
    "    )  # Output i: oi - oj <= u1 = ui - lj\n",
    "    aux_var = 0\n",
    "\n",
    "    for i, output in enumerate(output_variables):\n",
    "        if i != network_output:\n",
    "            ub = upper_bounds_diffs[i]\n",
    "            z = binary_variables[aux_var]\n",
    "            mdl.add_constraint(variable_output - output - ub * (1 - z) <= 0)\n",
    "            aux_var += 1\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def get_minimal_explanation(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    ") -> List[LinearConstraint]:\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\") # todo: como isso é utilizado dentro do insert_output_constraints_fischetti?\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None: \n",
    "            mdl.add_constraint(constraint)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n",
    "\n",
    "\n",
    "def get_explanation_relaxed(\n",
    "    mdl,\n",
    "    network_input,\n",
    "    network_output,\n",
    "    n_classes,\n",
    "    method,\n",
    "    output_bounds=None,\n",
    "    initial_explanation=None,\n",
    "    delta=0.1,\n",
    ") -> List[LinearConstraint]:\n",
    "    # todo: output_bounds só é relevante se o metodo for tjeng\n",
    "    assert not (\n",
    "        method == \"tjeng\" and output_bounds == None\n",
    "    ), \"If the method tjeng is chosen, output_bounds must be passed.\"\n",
    "\n",
    "    output_variables = [mdl.get_var_by_name(f\"o_{i}\") for i in range(n_classes)]\n",
    "\n",
    "    if initial_explanation is None:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == feature.numpy()\n",
    "                for i, feature in enumerate(network_input[0])\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "    else:\n",
    "        input_constraints = mdl.add_constraints(\n",
    "            [\n",
    "                mdl.get_var_by_name(f\"x_{i}\") == network_input[0][i].numpy()\n",
    "                for i in initial_explanation\n",
    "            ],\n",
    "            names=\"input\",\n",
    "        )\n",
    "\n",
    "    binary_variables = mdl.binary_var_list(n_classes - 1, name=\"b\")\n",
    "    mdl.add_constraint(mdl.sum(binary_variables) >= 1)\n",
    "\n",
    "    if method == \"tjeng\":\n",
    "        mdl = insert_output_constraints_tjeng(\n",
    "            mdl, output_variables, network_output, binary_variables, output_bounds\n",
    "        )\n",
    "\n",
    "    # todo: !(o1>o2 and o1>o3)\n",
    "    # todo: modificar para o1<=o2 or o1<=o3\n",
    "    else:\n",
    "        mdl = insert_output_constraints_fischetti(\n",
    "            mdl, output_variables, network_output, binary_variables\n",
    "        )\n",
    "\n",
    "    for constraint in input_constraints:\n",
    "        mdl.remove_constraint(constraint)\n",
    "\n",
    "        x = constraint.get_left_expr()\n",
    "        v = constraint.get_right_expr()\n",
    "\n",
    "        constraint_left = mdl.add_constraint(v - delta <= x)\n",
    "        constraint_right = mdl.add_constraint(x <= v + delta)\n",
    "\n",
    "        mdl.solve(log_output=False)\n",
    "        if mdl.solution is not None:\n",
    "            mdl.add_constraint(constraint)\n",
    "            mdl.remove_constraint(constraint_left)\n",
    "            mdl.remove_constraint(constraint_right)\n",
    "\n",
    "    return mdl.find_matching_linear_constraints(\"input\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    datasets = [  # {'dir_path': 'australian', 'n_classes': 2},\n",
    "        # {'dir_path': 'auto', 'n_classes': 5},\n",
    "        # {'dir_path': 'backache', 'n_classes': 2},\n",
    "        # {'dir_path': 'breast-cancer', 'n_classes': 2},\n",
    "        # {'dir_path': 'cleve', 'n_cla\n",
    "        # sses': 2},\n",
    "        # {'dir_path': 'cleveland', 'n_classes': 5},\n",
    "        # {'dir_path': 'glass', 'n_classes': 5},\n",
    "        {\"dir_path\": \"glass2\", \"n_classes\": 2},\n",
    "        # {'dir_path': 'heart-statlog', 'n_classes': 2}, {'dir_path': 'hepatitis', 'n_classes': 2},\n",
    "        # {'dir_path': 'spect', 'n_classes': 2},\n",
    "        # {'dir_path': 'voting', 'n_classes': 2}\n",
    "    ]\n",
    "\n",
    "    configurations = [  # {'method': 'fischetti', 'relaxe_constraints': True},\n",
    "        {\"method\": \"fischetti\", \"relaxe_constraints\": True},\n",
    "        # {'method': 'tjeng', 'relaxe_constraints': True},\n",
    "        {\"method\": \"tjeng\", \"relaxe_constraints\": False},\n",
    "    ]\n",
    "\n",
    "    df = {\n",
    "        \"fischetti\": {\n",
    "            True: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "            False: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "        },\n",
    "        \"tjeng\": {\n",
    "            True: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "            False: {\"size\": [], \"milp_time\": [], \"build_time\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dir_path = dataset[\"dir_path\"]\n",
    "        n_classes = dataset[\"n_classes\"]\n",
    "\n",
    "        for config in configurations:\n",
    "            print(dataset, config)\n",
    "\n",
    "            method = config[\"method\"]\n",
    "            relaxe_constraints = config[\"relaxe_constraints\"]\n",
    "\n",
    "            data_test = pd.read_csv(f\"datasets\\\\{dir_path}\\\\test.csv\")\n",
    "            data_train = pd.read_csv(f\"datasets\\\\{dir_path}\\\\train.csv\")\n",
    "            data = data_train._append(data_test)\n",
    "\n",
    "            model_path = f\"datasets\\\\{dir_path}\\\\model_4layers_{dir_path}.h5\"\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            codify_network_time = []\n",
    "            for _ in range(10):\n",
    "                start = time()\n",
    "                mdl, output_bounds = codify_network(\n",
    "                    model, data, method, relaxe_constraints\n",
    "                )\n",
    "                codify_network_time.append(time() - start)\n",
    "                print(codify_network_time[-1])\n",
    "\n",
    "            time_list = []\n",
    "            len_list = []\n",
    "            # data = data.to_numpy()\n",
    "            data = data_test.to_numpy()\n",
    "            for i in range(data.shape[0]):\n",
    "                # if i % 50 == 0:\n",
    "                print(i)\n",
    "                network_input = data[i, :-1]\n",
    "\n",
    "                network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "                network_output = model.predict(tf.constant(network_input))[0]\n",
    "                network_output = tf.argmax(network_output)\n",
    "\n",
    "                mdl_aux = mdl.clone()\n",
    "                start = time()\n",
    "\n",
    "                explanation = get_minimal_explanation(\n",
    "                    mdl_aux,\n",
    "                    network_input,\n",
    "                    network_output,\n",
    "                    n_classes=n_classes,\n",
    "                    method=method,\n",
    "                    output_bounds=output_bounds,\n",
    "                )\n",
    "\n",
    "                time_list.append(time() - start)\n",
    "\n",
    "                len_list.append(len(explanation))\n",
    "\n",
    "            df[method][relaxe_constraints][\"size\"].extend(\n",
    "                [min(len_list), f\"{mean(len_list)} +- {stdev(len_list)}\", max(len_list)]\n",
    "            )\n",
    "            df[method][relaxe_constraints][\"milp_time\"].extend(\n",
    "                [\n",
    "                    min(time_list),\n",
    "                    f\"{mean(time_list)} +- {stdev(time_list)}\",\n",
    "                    max(time_list),\n",
    "                ]\n",
    "            )\n",
    "            df[method][relaxe_constraints][\"build_time\"].extend(\n",
    "                [\n",
    "                    min(codify_network_time),\n",
    "                    f\"{mean(codify_network_time)} +- {stdev(codify_network_time)}\",\n",
    "                    max(codify_network_time),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Explication sizes:\\nm: {min(len_list)}\\na: {mean(len_list)} +- {stdev(len_list)}\\nM: {max(len_list)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Time:\\nm: {min(time_list)}\\na: {mean(time_list)} +- {stdev(time_list)}\\nM: {max(time_list)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Build Time:\\nm: {min(codify_network_time)}\\na: {mean(codify_network_time)} +- {stdev(codify_network_time)}\\nM: {max(codify_network_time)}\"\n",
    "            )\n",
    "    \"a\" + 1\n",
    "    df = {\n",
    "        \"fischetti_relaxe_size\": df[\"fischetti\"][True][\"size\"],\n",
    "        \"fischetti_relaxe_time\": df[\"fischetti\"][True][\"milp_time\"],\n",
    "        \"fischetti_relaxe_build_time\": df[\"fischetti\"][True][\"build_time\"],\n",
    "        \"fischetti_not_relaxe_size\": df[\"fischetti\"][False][\"size\"],\n",
    "        \"fischetti_not_relaxe_time\": df[\"fischetti\"][False][\"milp_time\"],\n",
    "        \"fischetti_not_relaxe_build_time\": df[\"fischetti\"][False][\"build_time\"],\n",
    "        \"tjeng_relaxe_size\": df[\"tjeng\"][True][\"size\"],\n",
    "        \"tjeng_relaxe_time\": df[\"tjeng\"][True][\"milp_time\"],\n",
    "        \"tjeng_relaxe_build_time\": df[\"tjeng\"][True][\"build_time\"],\n",
    "        \"tjeng_not_relaxe_size\": df[\"tjeng\"][False][\"size\"],\n",
    "        \"tjeng_not_relaxe_time\": df[\"tjeng\"][False][\"milp_time\"],\n",
    "        \"tjeng_not_relaxe_build_time\": df[\"tjeng\"][False][\"build_time\"],\n",
    "    }\n",
    "\n",
    "    index_label = []\n",
    "    for dataset in datasets:\n",
    "        index_label.extend(\n",
    "            [\n",
    "                f\"{dataset['dir_path']}_m\",\n",
    "                f\"{dataset['dir_path']}_a\",\n",
    "                f\"{dataset['dir_path']}_M\",\n",
    "            ]\n",
    "        )\n",
    "    df = pd.DataFrame(data=df, index=index_label)\n",
    "    df.to_csv(\"results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from milp import codify_network, codify_network_relaxed\n",
    "from teste import get_explanation_relaxed, get_minimal_explanation\n",
    "from typing import List\n",
    "from docplex.mp.constr import LinearConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede(dir_path: str, num_classes: int, n_neurons: int, n_hidden_layers: int):\n",
    "    data_train = pd.read_csv(dir_path + \"\\\\\" + \"train.csv\").to_numpy()\n",
    "    data_test = pd.read_csv(dir_path + \"\\\\\" + \"test.csv\").to_numpy()\n",
    "\n",
    "    x_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "    x_test, y_test = data_test[:, :-1], data_test[:, -1]\n",
    "\n",
    "    y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=[x_train.shape[1]]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        dir_path, \"models\", f\"model_{n_hidden_layers}layers_{n_neurons}neurons.h5\"\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    ck = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor=\"val_accuracy\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train_ohe,\n",
    "        batch_size=4,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test_ohe),\n",
    "        verbose=2,\n",
    "        callbacks=[ck, es],\n",
    "    )\n",
    "    print(f\"Tempo de Treinamento: {time()-start}\")\n",
    "\n",
    "    # salvar modelo\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # avaliar modelo com os dados de treinamento\n",
    "    print(\"Resultado Treinamento\")\n",
    "    model.evaluate(x_train, y_train_ohe, verbose=2)\n",
    "\n",
    "    # avaliar modelo com os dados de teste\n",
    "    print(\"Resultado Teste\")\n",
    "    model.evaluate(x_test, y_test_ohe, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede_com_dataset_iris(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\iris\"\n",
    "    num_classes = 3\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_digits(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\digits\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_wine(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\wine\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(\n",
    "    dataset: {}, configuration: {}, instance_index: int\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = codify_network(model, data, method, relaxe_constraints)\n",
    "    \n",
    "    # todo: ao inves de receber um, receber varios a serem explicados sem ter que codificar uma nova rede\n",
    "    # todo: salvar a rede de alguma forma para reutilizar\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "    mdl_aux = mdl_milp_with_binary_variable.clone()\n",
    "\n",
    "    explanation = get_minimal_explanation(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables,\n",
    "    )\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance_relaxed(\n",
    "    dataset: {}, configuration: {}, instance_index: int, delta = 1\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    (\n",
    "        mdl_milp_with_binary_variable,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds,\n",
    "    ) = codify_network(model, data, method, relaxe_constraints)\n",
    "\n",
    "    model_milp_relaxed, output_bounds_relaxed = codify_network_relaxed(\n",
    "        model,\n",
    "        data,\n",
    "        method,\n",
    "        relaxe_constraints,\n",
    "        output_bounds_binary_variables,\n",
    "        bounds=bounds,\n",
    "    )\n",
    "\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    # print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "\n",
    "    mdl_aux = model_milp_relaxed.clone()\n",
    "\n",
    "    explanation = get_explanation_relaxed(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables, # output_bounds_binary_variables == output_bounds_relaxed\n",
    "        delta = delta\n",
    "    )\n",
    "\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explicar_rede():\n",
    "#     datasets = [\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/digits\",\n",
    "#             \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "#             \"n_classes\": 10,\n",
    "#         },\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/iris\",\n",
    "#             \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "#             \"n_classes\": 3,\n",
    "#         },\n",
    "#         {\n",
    "#             \"dir_path\": \"datasets/iris\",\n",
    "#             \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "#             \"n_classes\": 3,\n",
    "#         },\n",
    "#     ]\n",
    "#     configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n",
    "    \n",
    "#     dataset_index = 0\n",
    "\n",
    "#     for i in range(0, 1):\n",
    "#         print(\"binaria\", end = '')\n",
    "#         explanation = explain_instance(\n",
    "#             dataset=datasets[dataset_index], configuration=configurations[0], instance_index=i\n",
    "#         )\n",
    "\n",
    "#         # for x in explanation:\n",
    "#         #     print(x)\n",
    "#         print(\"len: \", len(explanation), \"\\n\")\n",
    "\n",
    "        \n",
    "#         print(\"relaxada\", end = '')\n",
    "#         explanation = explain_instance_relaxed(\n",
    "#             dataset=datasets[dataset_index], configuration=configurations[0], instance_index=i, delta = 0.5\n",
    "#         )\n",
    "\n",
    "#         # for x in explanation:\n",
    "#         #     print(x)\n",
    "#         print(\"len: \", len(explanation), \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicar_rede()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "        {\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "    ]\n",
    "configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 687ms/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 416ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 400ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# instance_index = 0\n",
    "dataset_index = 1\n",
    "resultados = pd.DataFrame(columns=[\"instance_index\",\"tempo_original\", \"tempo_relaxado\", \"len_original\", \"len_relaxado\"])\n",
    "for instance_index in range(50):\n",
    "  # explain_instance\n",
    "  start_time = time.time()\n",
    "  explanation = explain_instance(dataset=datasets[dataset_index], configuration=configurations[0], instance_index=instance_index)\n",
    "  end_time = time.time()\n",
    "  tempo_original = end_time - start_time\n",
    "  len_original = len(explanation)\n",
    "  # resultados.loc[len(resultados)] = [\"original\", tempo_execucao, comprimento_explicacao]\n",
    "  # explain_instance_relaxed\n",
    "  start_time = time.time()\n",
    "  explanation_relaxed = explain_instance_relaxed(dataset=datasets[dataset_index], configuration=configurations[0], instance_index=instance_index, delta=1)\n",
    "  end_time = time.time()\n",
    "  tempo_relaxado = end_time - start_time\n",
    "  len_relaxado = len(explanation_relaxed)\n",
    "  resultados.loc[len(resultados)] = [instance_index, tempo_original, tempo_relaxado, len_original, len_relaxado]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_index</th>\n",
       "      <th>tempo_original</th>\n",
       "      <th>tempo_relaxado</th>\n",
       "      <th>len_original</th>\n",
       "      <th>len_relaxado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.987588</td>\n",
       "      <td>7.896489</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.526494</td>\n",
       "      <td>1.300955</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.374288</td>\n",
       "      <td>1.387963</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.885468</td>\n",
       "      <td>1.777465</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.766202</td>\n",
       "      <td>2.270261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.279475</td>\n",
       "      <td>1.319461</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.106391</td>\n",
       "      <td>1.248865</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.119921</td>\n",
       "      <td>1.273670</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.469473</td>\n",
       "      <td>1.037241</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.060074</td>\n",
       "      <td>1.115201</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.254224</td>\n",
       "      <td>2.130358</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.398954</td>\n",
       "      <td>1.643936</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.870490</td>\n",
       "      <td>1.316509</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.214447</td>\n",
       "      <td>1.102138</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.123237</td>\n",
       "      <td>1.207480</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.052386</td>\n",
       "      <td>1.185073</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.285937</td>\n",
       "      <td>1.185115</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.388490</td>\n",
       "      <td>1.091004</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.109239</td>\n",
       "      <td>1.107481</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.156830</td>\n",
       "      <td>2.269637</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.159404</td>\n",
       "      <td>1.352535</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.178816</td>\n",
       "      <td>1.210001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.375875</td>\n",
       "      <td>2.010303</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.092938</td>\n",
       "      <td>0.982633</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.942697</td>\n",
       "      <td>1.189276</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.277193</td>\n",
       "      <td>1.149536</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1.171486</td>\n",
       "      <td>1.229200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.169508</td>\n",
       "      <td>1.443652</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.312778</td>\n",
       "      <td>1.247918</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.185507</td>\n",
       "      <td>1.142304</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.0</td>\n",
       "      <td>1.145328</td>\n",
       "      <td>1.213286</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1.359491</td>\n",
       "      <td>1.167551</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1.289639</td>\n",
       "      <td>1.700644</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.159361</td>\n",
       "      <td>1.786290</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1.678682</td>\n",
       "      <td>1.347543</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.623195</td>\n",
       "      <td>1.193476</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.539459</td>\n",
       "      <td>1.419372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.152237</td>\n",
       "      <td>1.207572</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.146609</td>\n",
       "      <td>3.032803</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39.0</td>\n",
       "      <td>2.014397</td>\n",
       "      <td>1.364144</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.136962</td>\n",
       "      <td>1.105116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1.151353</td>\n",
       "      <td>1.154021</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1.122558</td>\n",
       "      <td>0.980084</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.206415</td>\n",
       "      <td>0.893982</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.133459</td>\n",
       "      <td>1.140927</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.417890</td>\n",
       "      <td>1.854527</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.204661</td>\n",
       "      <td>1.315616</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.157387</td>\n",
       "      <td>1.730386</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.601043</td>\n",
       "      <td>0.966217</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49.0</td>\n",
       "      <td>1.093157</td>\n",
       "      <td>0.991505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    instance_index  tempo_original  tempo_relaxado  len_original  len_relaxado\n",
       "0              0.0        6.987588        7.896489           3.0           4.0\n",
       "1              1.0        1.526494        1.300955           3.0           4.0\n",
       "2              2.0        1.374288        1.387963           3.0           4.0\n",
       "3              3.0        1.885468        1.777465           3.0           4.0\n",
       "4              4.0        1.766202        2.270261           3.0           4.0\n",
       "5              5.0        1.279475        1.319461           3.0           4.0\n",
       "6              6.0        1.106391        1.248865           3.0           4.0\n",
       "7              7.0        1.119921        1.273670           3.0           4.0\n",
       "8              8.0        1.469473        1.037241           3.0           4.0\n",
       "9              9.0        1.060074        1.115201           4.0           4.0\n",
       "10            10.0        1.254224        2.130358           3.0           4.0\n",
       "11            11.0        1.398954        1.643936           3.0           4.0\n",
       "12            12.0        1.870490        1.316509           3.0           4.0\n",
       "13            13.0        1.214447        1.102138           3.0           4.0\n",
       "14            14.0        1.123237        1.207480           2.0           4.0\n",
       "15            15.0        1.052386        1.185073           3.0           4.0\n",
       "16            16.0        1.285937        1.185115           3.0           4.0\n",
       "17            17.0        1.388490        1.091004           3.0           4.0\n",
       "18            18.0        1.109239        1.107481           3.0           4.0\n",
       "19            19.0        1.156830        2.269637           3.0           4.0\n",
       "20            20.0        1.159404        1.352535           3.0           4.0\n",
       "21            21.0        1.178816        1.210001           3.0           4.0\n",
       "22            22.0        1.375875        2.010303           3.0           4.0\n",
       "23            23.0        1.092938        0.982633           3.0           4.0\n",
       "24            24.0        0.942697        1.189276           3.0           4.0\n",
       "25            25.0        1.277193        1.149536           3.0           4.0\n",
       "26            26.0        1.171486        1.229200           3.0           4.0\n",
       "27            27.0        1.169508        1.443652           3.0           4.0\n",
       "28            28.0        1.312778        1.247918           3.0           4.0\n",
       "29            29.0        1.185507        1.142304           3.0           4.0\n",
       "30            30.0        1.145328        1.213286           3.0           4.0\n",
       "31            31.0        1.359491        1.167551           3.0           4.0\n",
       "32            32.0        1.289639        1.700644           3.0           4.0\n",
       "33            33.0        1.159361        1.786290           3.0           4.0\n",
       "34            34.0        1.678682        1.347543           3.0           4.0\n",
       "35            35.0        1.623195        1.193476           3.0           4.0\n",
       "36            36.0        1.539459        1.419372           3.0           4.0\n",
       "37            37.0        1.152237        1.207572           3.0           4.0\n",
       "38            38.0        1.146609        3.032803           3.0           4.0\n",
       "39            39.0        2.014397        1.364144           4.0           4.0\n",
       "40            40.0        1.136962        1.105116           4.0           4.0\n",
       "41            41.0        1.151353        1.154021           3.0           4.0\n",
       "42            42.0        1.122558        0.980084           3.0           4.0\n",
       "43            43.0        1.206415        0.893982           4.0           4.0\n",
       "44            44.0        1.133459        1.140927           3.0           4.0\n",
       "45            45.0        1.417890        1.854527           3.0           4.0\n",
       "46            46.0        1.204661        1.315616           3.0           4.0\n",
       "47            47.0        1.157387        1.730386           3.0           4.0\n",
       "48            48.0        1.601043        0.966217           3.0           4.0\n",
       "49            49.0        1.093157        0.991505           3.0           4.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_do_arquivo = \"dataset_iris_resultados.csv\"\n",
    "# caminho_do_diretorio = \"/caminho/do/seu/diretorio\"\n",
    "caminho_do_diretorio = \"C:\\\\Users\\\\mylle\\\\OneDrive\\\\Documentos\\\\GitHub\\\\TestesTypescript\\\\Explications-ANNs\"\n",
    "caminho_completo = f\"{caminho_do_diretorio}/{nome_do_arquivo}\"\n",
    "\n",
    "# Salve o DataFrame como CSV\n",
    "resultados.to_csv(caminho_completo, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
