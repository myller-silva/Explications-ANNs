{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from milp import codify_network, codify_network_relaxed\n",
    "from teste import get_explanation_relaxed, get_minimal_explanation\n",
    "from typing import List\n",
    "from docplex.mp.constr import LinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede(dir_path: str, num_classes: int, n_neurons: int, n_hidden_layers: int):\n",
    "    data_train = pd.read_csv(dir_path + \"\\\\\" + \"train.csv\").to_numpy()\n",
    "    data_test = pd.read_csv(dir_path + \"\\\\\" + \"test.csv\").to_numpy()\n",
    "\n",
    "    x_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "    x_test, y_test = data_test[:, :-1], data_test[:, -1]\n",
    "\n",
    "    y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    y_test_ohe = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=[x_train.shape[1]]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        dir_path, \"models\", f\"model_{n_hidden_layers}layers_{n_neurons}neurons.h5\"\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    ck = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor=\"val_accuracy\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train_ohe,\n",
    "        batch_size=4,\n",
    "        epochs=100,\n",
    "        validation_data=(x_test, y_test_ohe),\n",
    "        verbose=2,\n",
    "        callbacks=[ck, es],\n",
    "    )\n",
    "    print(f\"Tempo de Treinamento: {time()-start}\")\n",
    "\n",
    "    # salvar modelo\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # avaliar modelo com os dados de treinamento\n",
    "    print(\"Resultado Treinamento\")\n",
    "    model.evaluate(x_train, y_train_ohe, verbose=2)\n",
    "\n",
    "    # avaliar modelo com os dados de teste\n",
    "    print(\"Resultado Teste\")\n",
    "    model.evaluate(x_test, y_test_ohe, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(\n",
    "    dataset: {}, configuration: {}, instance_index: int\n",
    ") -> List[LinearConstraint]:\n",
    "    dir_path, n_classes, model = (\n",
    "        dataset[\"dir_path\"],\n",
    "        dataset[\"n_classes\"],\n",
    "        dataset[\"model\"],\n",
    "    )\n",
    "\n",
    "    method = configuration[\"method\"]\n",
    "    relaxe_constraints = configuration[\"relaxe_constraints\"]\n",
    "\n",
    "    data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n",
    "    data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n",
    "\n",
    "    data = data_train._append(data_test)\n",
    "\n",
    "    model = tf.keras.models.load_model(f\"{dir_path}/{model}\")\n",
    "\n",
    "    # todo: modificar\n",
    "    # mdl, output_bounds = codify_network(model, data, method, relaxe_constraints)\n",
    "    mdl_milp_with_binary_variable, output_bounds_binary_variables, bounds_binary_variables = codify_network(\n",
    "        model, data, method, relaxe_constraints\n",
    "    )\n",
    "\n",
    "    # usar bounds precisos do primeiro modelo\n",
    "    model_milp_relaxed, output_bounds_relaxed = codify_network_relaxed(\n",
    "        model, data, method, relaxe_constraints, output_bounds_binary_variables, bounds = bounds_binary_variables\n",
    "    )\n",
    "\n",
    "    network_input = data.iloc[instance_index, :-1]\n",
    "    print(network_input)  # network_input = instance\n",
    "\n",
    "    network_input = tf.reshape(tf.constant(network_input), (1, -1))\n",
    "\n",
    "    network_output = model.predict(tf.constant(network_input))[0]\n",
    "\n",
    "    network_output = tf.argmax(network_output)\n",
    "\n",
    "    mdl_aux = model_milp_relaxed.clone() # todo: testar depois com mdl_milp_with_binary_variable \n",
    "\n",
    "    explanation = get_explanation_relaxed(\n",
    "        mdl_aux,\n",
    "        network_input,\n",
    "        network_output,\n",
    "        n_classes=n_classes,\n",
    "        method=method,\n",
    "        output_bounds=output_bounds_binary_variables,\n",
    "        delta = 0\n",
    "    )\n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_rede_com_dataset_iris(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\iris\"\n",
    "    num_classes = 3\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n",
    "\n",
    "\n",
    "def gerar_rede_com_dataset_digits(n_neurons=20, n_hidden_layers=1):\n",
    "    dir_path = \"datasets\\\\digits\"\n",
    "    num_classes = 10\n",
    "    gerar_rede(dir_path, num_classes, n_neurons, n_hidden_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicar_rede():\n",
    "    datasets = [\n",
    "        {\n",
    "            \"dir_path\": \"datasets/digits\",\n",
    "            \"model\": \"models/model_5layers_20neurons.h5\",\n",
    "            \"n_classes\": 10,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_1layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"dir_path\": \"datasets/iris\",\n",
    "            \"model\": \"models/model_6layers_20neurons.h5\",\n",
    "            \"n_classes\": 3,\n",
    "        },\n",
    "    ]\n",
    "    configurations = [{\"method\": \"fischetti\", \"relaxe_constraints\": True}]\n",
    "\n",
    "    for i in range(0, 1):\n",
    "        explanation = explain_instance(\n",
    "            dataset=datasets[1], configuration=configurations[0], instance_index=i\n",
    "        )\n",
    "\n",
    "    for x in explanation:\n",
    "        print(x)\n",
    "    print(\"len: \", len(explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6122007369995119, 1.6778421103954317]\n",
      "[-1.0509401857852936, 1.2092040181159973]\n",
      "[-0.868502140045166, 0.32010936737060547]\n",
      "[-0.21047664433717728, 2.1316187977790833]\n",
      "[-0.27645808085799217, 1.6028032936155796]\n",
      "[-0.27311925403773785, 0.8610581457614898]\n",
      "[-0.5554660968482494, 0.21914495155215263]\n",
      "[-0.4563867449760437, 0.7460575252771378]\n",
      "[-0.8218722145767211, 0.07872235774993896]\n",
      "[-0.851436048746109, 1.2469268441200256]\n",
      "[-0.5933456420898438, 0.9865902364253999]\n",
      "[-0.3293287009000778, 1.5459568053483963]\n",
      "[-0.2331935614347458, 1.6790897846221924]\n",
      "[-0.5084999693198043, 0.22432684525847438]\n",
      "[-0.2245442429557443, 1.138442151248455]\n",
      "[-0.3766915798187256, 1.0976879671216009]\n",
      "[-0.9108107537031174, 0.41926206648349756]\n",
      "[-0.5208602622151375, 0.3270818774666865]\n",
      "[-0.6625148589096069, 0.0]\n",
      "[-0.33704200387001043, 1.2212621569633484]\n",
      "[-0.6122007369995119, 1.6778421103954317]\n",
      "[-1.0509401857852936, 1.2092040181159973]\n",
      "[-0.868502140045166, 0.32010936737060547]\n",
      "sepal_length    0.138889\n",
      "sepal_width     0.416667\n",
      "petal_length    0.067797\n",
      "petal_width     0.083333\n",
      "Name: 0, dtype: float64\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025AF73A42C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "input1: x_0 == 0.1388888888888888\n",
      "input2: x_1 == 0.4166666666666667\n",
      "input3: x_2 == 0.0677966101694915\n",
      "input4: x_3 == 0.0833333333333333\n",
      "len:  4\n"
     ]
    }
   ],
   "source": [
    "explicar_rede()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
